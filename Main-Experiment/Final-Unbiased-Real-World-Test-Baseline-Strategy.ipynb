{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b87656f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/veeranonthuvasin/Desktop/MSc-Data-Science-Bristol/Dissertation/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "import timm\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "import glob\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "960d4cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>DayOfBirth</th>\n",
       "      <th>EducationalDegree</th>\n",
       "      <th>Gender</th>\n",
       "      <th>NativeCountry</th>\n",
       "      <th>NativeLanguage</th>\n",
       "      <th>OtherLanguage</th>\n",
       "      <th>Profession</th>\n",
       "      <th>WritingType</th>\n",
       "      <th>Science</th>\n",
       "      <th>WrittenLanguage</th>\n",
       "      <th>ascii_path</th>\n",
       "      <th>images_path</th>\n",
       "      <th>stroke_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>French</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['./Data/Bristol-Corpus/Real-World-GrayScale-P...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>German</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['./Data/Bristol-Corpus/Real-World-GrayScale-P...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>German</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['./Data/Bristol-Corpus/Real-World-GrayScale-P...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>French</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['./Data/Bristol-Corpus/Real-World-GrayScale-P...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>French</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['./Data/Bristol-Corpus/Real-World-GrayScale-P...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>German</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['./Data/Bristol-Corpus/Real-World-GrayScale-P...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name  DayOfBirth  EducationalDegree  Gender  NativeCountry NativeLanguage  \\\n",
       "0     0         NaN                NaN     NaN            NaN         French   \n",
       "1     1         NaN                NaN     NaN            NaN         German   \n",
       "2     3         NaN                NaN     NaN            NaN         German   \n",
       "3     4         NaN                NaN     NaN            NaN         French   \n",
       "4     5         NaN                NaN     NaN            NaN         French   \n",
       "5     6         NaN                NaN     NaN            NaN         German   \n",
       "\n",
       "   OtherLanguage  Profession  WritingType  Science  WrittenLanguage  \\\n",
       "0            NaN         NaN          NaN      NaN              NaN   \n",
       "1            NaN         NaN          NaN      NaN              NaN   \n",
       "2            NaN         NaN          NaN      NaN              NaN   \n",
       "3            NaN         NaN          NaN      NaN              NaN   \n",
       "4            NaN         NaN          NaN      NaN              NaN   \n",
       "5            NaN         NaN          NaN      NaN              NaN   \n",
       "\n",
       "   ascii_path                                        images_path  stroke_path  \n",
       "0         NaN  ['./Data/Bristol-Corpus/Real-World-GrayScale-P...          NaN  \n",
       "1         NaN  ['./Data/Bristol-Corpus/Real-World-GrayScale-P...          NaN  \n",
       "2         NaN  ['./Data/Bristol-Corpus/Real-World-GrayScale-P...          NaN  \n",
       "3         NaN  ['./Data/Bristol-Corpus/Real-World-GrayScale-P...          NaN  \n",
       "4         NaN  ['./Data/Bristol-Corpus/Real-World-GrayScale-P...          NaN  \n",
       "5         NaN  ['./Data/Bristol-Corpus/Real-World-GrayScale-P...          NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_world_data = pd.read_csv('../Data/Bristol-Corpus/Real-World-GrayScale-PSM3/real_world_df.csv')\n",
    "real_world_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed8f4577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NativeLanguage\n",
       "French    3\n",
       "German    3\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_world_data['NativeLanguage'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ea509b",
   "metadata": {},
   "source": [
    "# Utility Functions\n",
    "- Feature Extraction Functions\n",
    "- Change writer label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b804270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(writer_name, images_path):\n",
    "  model = timm.create_model(\n",
    "      'convnextv2_nano.fcmae_ft_in22k_in1k',\n",
    "      pretrained=True,\n",
    "      num_classes=0,  # remove classifier nn.Linear\n",
    "  )\n",
    "  model = model.eval()\n",
    "\n",
    "  data_config = timm.data.resolve_model_data_config(model)\n",
    "  transforms = timm.data.create_transform(**data_config, is_training=False)\n",
    "\n",
    "  all_features_data = []\n",
    "  writer_id_list = []\n",
    "  writer_forms_list = []\n",
    "  for name, i in tqdm(zip(writer_name,images_path)):\n",
    "      i = ast.literal_eval(i)\n",
    "      for j in i:\n",
    "        j = j.replace('./', '../')\n",
    "        image_list = glob.glob(j)\n",
    "        for k in image_list:\n",
    "          with Image.open(k) as img:\n",
    "            img = img.convert('RGB')\n",
    "            \n",
    "            with torch.no_grad():\n",
    "              output = model(transforms(img).unsqueeze(0))\n",
    "            # print(output)\n",
    "            # features = output.pooler_output.detach().numpy()\n",
    "            # print(features)\n",
    "            # last_hidden_states = outputs.last_hidden_state\n",
    "            # print(last_hidden_states.shape)\n",
    "            # features = last_hidden_states[:, 0, :]\n",
    "            # print(features)\n",
    "            # Store the results\n",
    "            image_form = os.path.splitext(os.path.basename(k))[0]\n",
    "            writer_forms_list.append(image_form)\n",
    "            writer_id_list.append(name)\n",
    "            all_features_data.append(output.detach().flatten().tolist())\n",
    "\n",
    "              \n",
    "\n",
    "\n",
    "  writer_features_df = pd.DataFrame(data=all_features_data)\n",
    "  writer_features_df['name'] = writer_id_list\n",
    "  writer_features_df['form'] = writer_forms_list\n",
    "  return writer_features_df\n",
    "\n",
    "def convert_y(y):\n",
    "  if y == 'German':\n",
    "    return 0\n",
    "  if y == 'French': \n",
    "    return 1\n",
    "  if y == 'English':\n",
    "    return 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f7d9e2",
   "metadata": {},
   "source": [
    "# Feature Extraction Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "704f4066",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:17,  2.86s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>632</th>\n",
       "      <th>633</th>\n",
       "      <th>634</th>\n",
       "      <th>635</th>\n",
       "      <th>636</th>\n",
       "      <th>637</th>\n",
       "      <th>638</th>\n",
       "      <th>639</th>\n",
       "      <th>name</th>\n",
       "      <th>form</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.900881</td>\n",
       "      <td>-1.535855</td>\n",
       "      <td>1.216326</td>\n",
       "      <td>0.263924</td>\n",
       "      <td>0.861893</td>\n",
       "      <td>0.184795</td>\n",
       "      <td>-1.468211</td>\n",
       "      <td>-0.903002</td>\n",
       "      <td>0.471242</td>\n",
       "      <td>-1.091495</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058538</td>\n",
       "      <td>-1.304109</td>\n",
       "      <td>1.541442</td>\n",
       "      <td>-0.424794</td>\n",
       "      <td>-0.802706</td>\n",
       "      <td>1.041371</td>\n",
       "      <td>-0.328758</td>\n",
       "      <td>3.136823</td>\n",
       "      <td>0</td>\n",
       "      <td>page000_line003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.134265</td>\n",
       "      <td>-1.362282</td>\n",
       "      <td>2.273987</td>\n",
       "      <td>0.284600</td>\n",
       "      <td>-0.652768</td>\n",
       "      <td>-0.222237</td>\n",
       "      <td>-0.832183</td>\n",
       "      <td>-1.145109</td>\n",
       "      <td>0.296855</td>\n",
       "      <td>-2.368747</td>\n",
       "      <td>...</td>\n",
       "      <td>0.157107</td>\n",
       "      <td>-1.092878</td>\n",
       "      <td>1.892547</td>\n",
       "      <td>-0.487107</td>\n",
       "      <td>0.093969</td>\n",
       "      <td>2.060857</td>\n",
       "      <td>-1.213497</td>\n",
       "      <td>2.975895</td>\n",
       "      <td>0</td>\n",
       "      <td>page000_line002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.593472</td>\n",
       "      <td>-0.976235</td>\n",
       "      <td>0.744769</td>\n",
       "      <td>0.308163</td>\n",
       "      <td>-0.685127</td>\n",
       "      <td>-0.755348</td>\n",
       "      <td>-2.012943</td>\n",
       "      <td>0.673236</td>\n",
       "      <td>-0.896982</td>\n",
       "      <td>-1.988740</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.753516</td>\n",
       "      <td>-0.608883</td>\n",
       "      <td>1.339883</td>\n",
       "      <td>0.432048</td>\n",
       "      <td>0.636367</td>\n",
       "      <td>1.714106</td>\n",
       "      <td>0.504608</td>\n",
       "      <td>1.266553</td>\n",
       "      <td>0</td>\n",
       "      <td>page000_line000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.280957</td>\n",
       "      <td>-1.285501</td>\n",
       "      <td>1.829224</td>\n",
       "      <td>0.499230</td>\n",
       "      <td>-0.052572</td>\n",
       "      <td>0.709289</td>\n",
       "      <td>-1.074080</td>\n",
       "      <td>-1.166486</td>\n",
       "      <td>0.981797</td>\n",
       "      <td>-1.591591</td>\n",
       "      <td>...</td>\n",
       "      <td>0.361287</td>\n",
       "      <td>-0.495855</td>\n",
       "      <td>1.635529</td>\n",
       "      <td>-0.772384</td>\n",
       "      <td>1.012281</td>\n",
       "      <td>2.350589</td>\n",
       "      <td>-0.735705</td>\n",
       "      <td>2.450419</td>\n",
       "      <td>0</td>\n",
       "      <td>page000_line001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.312774</td>\n",
       "      <td>-0.413435</td>\n",
       "      <td>2.063291</td>\n",
       "      <td>0.887190</td>\n",
       "      <td>0.801703</td>\n",
       "      <td>-0.023718</td>\n",
       "      <td>-2.104139</td>\n",
       "      <td>0.721972</td>\n",
       "      <td>0.649561</td>\n",
       "      <td>-2.637098</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.471062</td>\n",
       "      <td>0.093955</td>\n",
       "      <td>0.671852</td>\n",
       "      <td>0.130571</td>\n",
       "      <td>-0.747895</td>\n",
       "      <td>2.493425</td>\n",
       "      <td>-1.339399</td>\n",
       "      <td>1.875247</td>\n",
       "      <td>0</td>\n",
       "      <td>page000_line005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.088423</td>\n",
       "      <td>-0.558079</td>\n",
       "      <td>2.144183</td>\n",
       "      <td>0.348551</td>\n",
       "      <td>-0.823501</td>\n",
       "      <td>-0.331861</td>\n",
       "      <td>-1.095646</td>\n",
       "      <td>0.077632</td>\n",
       "      <td>0.161654</td>\n",
       "      <td>-2.837906</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.645933</td>\n",
       "      <td>-0.706350</td>\n",
       "      <td>0.919395</td>\n",
       "      <td>1.023801</td>\n",
       "      <td>-0.501715</td>\n",
       "      <td>1.016948</td>\n",
       "      <td>-1.168648</td>\n",
       "      <td>1.912641</td>\n",
       "      <td>0</td>\n",
       "      <td>page000_line004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.352457</td>\n",
       "      <td>-1.241885</td>\n",
       "      <td>1.946865</td>\n",
       "      <td>0.645203</td>\n",
       "      <td>-0.915543</td>\n",
       "      <td>-0.268871</td>\n",
       "      <td>-1.386410</td>\n",
       "      <td>-0.908995</td>\n",
       "      <td>0.262660</td>\n",
       "      <td>-2.861198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164378</td>\n",
       "      <td>0.277399</td>\n",
       "      <td>2.058115</td>\n",
       "      <td>0.161064</td>\n",
       "      <td>0.208770</td>\n",
       "      <td>3.003321</td>\n",
       "      <td>-1.318584</td>\n",
       "      <td>2.281312</td>\n",
       "      <td>0</td>\n",
       "      <td>page000_line006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.507467</td>\n",
       "      <td>-1.915475</td>\n",
       "      <td>2.334567</td>\n",
       "      <td>0.967679</td>\n",
       "      <td>0.755115</td>\n",
       "      <td>0.768464</td>\n",
       "      <td>-1.766531</td>\n",
       "      <td>-0.953910</td>\n",
       "      <td>0.261470</td>\n",
       "      <td>-2.457000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.438582</td>\n",
       "      <td>-0.710739</td>\n",
       "      <td>1.944403</td>\n",
       "      <td>-0.137118</td>\n",
       "      <td>-0.349195</td>\n",
       "      <td>2.496340</td>\n",
       "      <td>-0.891258</td>\n",
       "      <td>2.711566</td>\n",
       "      <td>0</td>\n",
       "      <td>page000_line007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.369867</td>\n",
       "      <td>-1.144850</td>\n",
       "      <td>2.374497</td>\n",
       "      <td>1.486444</td>\n",
       "      <td>0.057034</td>\n",
       "      <td>0.659323</td>\n",
       "      <td>-0.906150</td>\n",
       "      <td>-0.305979</td>\n",
       "      <td>-0.087208</td>\n",
       "      <td>-2.945641</td>\n",
       "      <td>...</td>\n",
       "      <td>0.621893</td>\n",
       "      <td>-1.044549</td>\n",
       "      <td>1.939120</td>\n",
       "      <td>0.076557</td>\n",
       "      <td>0.389761</td>\n",
       "      <td>1.469566</td>\n",
       "      <td>-0.691275</td>\n",
       "      <td>1.780019</td>\n",
       "      <td>0</td>\n",
       "      <td>page000_line008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.611749</td>\n",
       "      <td>-2.095418</td>\n",
       "      <td>0.496747</td>\n",
       "      <td>1.899151</td>\n",
       "      <td>-0.842998</td>\n",
       "      <td>0.447551</td>\n",
       "      <td>-1.261050</td>\n",
       "      <td>-2.167131</td>\n",
       "      <td>1.413064</td>\n",
       "      <td>-2.242526</td>\n",
       "      <td>...</td>\n",
       "      <td>0.244677</td>\n",
       "      <td>-0.928682</td>\n",
       "      <td>1.154003</td>\n",
       "      <td>-0.196702</td>\n",
       "      <td>-1.030584</td>\n",
       "      <td>1.451702</td>\n",
       "      <td>-0.573324</td>\n",
       "      <td>0.096913</td>\n",
       "      <td>1</td>\n",
       "      <td>page000_line003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-2.290432</td>\n",
       "      <td>-1.823981</td>\n",
       "      <td>-0.245025</td>\n",
       "      <td>1.480978</td>\n",
       "      <td>-0.356717</td>\n",
       "      <td>0.568426</td>\n",
       "      <td>-0.646507</td>\n",
       "      <td>-2.733148</td>\n",
       "      <td>2.006800</td>\n",
       "      <td>-2.515923</td>\n",
       "      <td>...</td>\n",
       "      <td>0.518083</td>\n",
       "      <td>0.408829</td>\n",
       "      <td>0.237417</td>\n",
       "      <td>0.417480</td>\n",
       "      <td>0.410241</td>\n",
       "      <td>1.516244</td>\n",
       "      <td>-0.792029</td>\n",
       "      <td>-1.057558</td>\n",
       "      <td>1</td>\n",
       "      <td>page000_line002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.207571</td>\n",
       "      <td>-1.161908</td>\n",
       "      <td>-0.629806</td>\n",
       "      <td>1.927256</td>\n",
       "      <td>0.069100</td>\n",
       "      <td>0.012873</td>\n",
       "      <td>-0.593543</td>\n",
       "      <td>-1.862664</td>\n",
       "      <td>1.482231</td>\n",
       "      <td>-1.305336</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.328434</td>\n",
       "      <td>-0.731346</td>\n",
       "      <td>0.970486</td>\n",
       "      <td>1.150844</td>\n",
       "      <td>-0.227760</td>\n",
       "      <td>0.252524</td>\n",
       "      <td>0.596436</td>\n",
       "      <td>-0.253897</td>\n",
       "      <td>1</td>\n",
       "      <td>page000_line000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-1.589233</td>\n",
       "      <td>-1.054851</td>\n",
       "      <td>-0.275232</td>\n",
       "      <td>2.352102</td>\n",
       "      <td>-1.657410</td>\n",
       "      <td>0.068928</td>\n",
       "      <td>-0.326524</td>\n",
       "      <td>-2.286921</td>\n",
       "      <td>1.303765</td>\n",
       "      <td>-1.600224</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040765</td>\n",
       "      <td>-0.186851</td>\n",
       "      <td>1.600801</td>\n",
       "      <td>-1.122975</td>\n",
       "      <td>-0.408834</td>\n",
       "      <td>3.436998</td>\n",
       "      <td>-1.169829</td>\n",
       "      <td>0.025704</td>\n",
       "      <td>1</td>\n",
       "      <td>page000_line001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.566000</td>\n",
       "      <td>-1.090448</td>\n",
       "      <td>0.432487</td>\n",
       "      <td>1.193386</td>\n",
       "      <td>-0.284967</td>\n",
       "      <td>0.979698</td>\n",
       "      <td>-1.883320</td>\n",
       "      <td>-2.088283</td>\n",
       "      <td>1.133648</td>\n",
       "      <td>-1.532070</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.282095</td>\n",
       "      <td>-0.349165</td>\n",
       "      <td>0.040628</td>\n",
       "      <td>0.709523</td>\n",
       "      <td>-0.784128</td>\n",
       "      <td>1.829252</td>\n",
       "      <td>-1.324068</td>\n",
       "      <td>0.003215</td>\n",
       "      <td>1</td>\n",
       "      <td>page000_line005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.757621</td>\n",
       "      <td>-0.894421</td>\n",
       "      <td>0.013984</td>\n",
       "      <td>2.179670</td>\n",
       "      <td>-1.608992</td>\n",
       "      <td>0.379539</td>\n",
       "      <td>-1.496733</td>\n",
       "      <td>-1.766940</td>\n",
       "      <td>1.191479</td>\n",
       "      <td>-2.852113</td>\n",
       "      <td>...</td>\n",
       "      <td>0.270399</td>\n",
       "      <td>-1.446855</td>\n",
       "      <td>0.904377</td>\n",
       "      <td>0.191518</td>\n",
       "      <td>-1.545192</td>\n",
       "      <td>2.216811</td>\n",
       "      <td>-1.141569</td>\n",
       "      <td>-1.106837</td>\n",
       "      <td>1</td>\n",
       "      <td>page000_line004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-1.511285</td>\n",
       "      <td>-0.159393</td>\n",
       "      <td>-1.285892</td>\n",
       "      <td>0.883206</td>\n",
       "      <td>-0.240112</td>\n",
       "      <td>-0.082843</td>\n",
       "      <td>-1.698460</td>\n",
       "      <td>-0.617277</td>\n",
       "      <td>0.631346</td>\n",
       "      <td>-0.851741</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.025800</td>\n",
       "      <td>-0.099167</td>\n",
       "      <td>0.538432</td>\n",
       "      <td>1.420944</td>\n",
       "      <td>-0.220729</td>\n",
       "      <td>1.039717</td>\n",
       "      <td>-0.968723</td>\n",
       "      <td>1.182505</td>\n",
       "      <td>3</td>\n",
       "      <td>page000_line003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.915069</td>\n",
       "      <td>0.126906</td>\n",
       "      <td>-1.253779</td>\n",
       "      <td>1.961727</td>\n",
       "      <td>-0.480075</td>\n",
       "      <td>0.358586</td>\n",
       "      <td>-1.083527</td>\n",
       "      <td>-0.397470</td>\n",
       "      <td>0.726015</td>\n",
       "      <td>-0.928387</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.101263</td>\n",
       "      <td>1.035422</td>\n",
       "      <td>1.443239</td>\n",
       "      <td>1.378848</td>\n",
       "      <td>0.203002</td>\n",
       "      <td>2.551162</td>\n",
       "      <td>-1.204188</td>\n",
       "      <td>1.095948</td>\n",
       "      <td>3</td>\n",
       "      <td>page000_line002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-1.399246</td>\n",
       "      <td>-0.449594</td>\n",
       "      <td>-0.050102</td>\n",
       "      <td>0.851918</td>\n",
       "      <td>-0.061686</td>\n",
       "      <td>0.489559</td>\n",
       "      <td>-1.947546</td>\n",
       "      <td>-0.186119</td>\n",
       "      <td>0.788533</td>\n",
       "      <td>-0.059928</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.674960</td>\n",
       "      <td>-0.008328</td>\n",
       "      <td>1.219678</td>\n",
       "      <td>0.704170</td>\n",
       "      <td>0.378878</td>\n",
       "      <td>1.588672</td>\n",
       "      <td>-0.762329</td>\n",
       "      <td>1.949395</td>\n",
       "      <td>3</td>\n",
       "      <td>page000_line001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.400874</td>\n",
       "      <td>-0.573764</td>\n",
       "      <td>0.262399</td>\n",
       "      <td>1.218095</td>\n",
       "      <td>-1.354946</td>\n",
       "      <td>-0.158789</td>\n",
       "      <td>-1.226880</td>\n",
       "      <td>-1.198591</td>\n",
       "      <td>1.332944</td>\n",
       "      <td>-0.620245</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.091506</td>\n",
       "      <td>-0.495805</td>\n",
       "      <td>1.265438</td>\n",
       "      <td>-0.169847</td>\n",
       "      <td>1.055912</td>\n",
       "      <td>0.814413</td>\n",
       "      <td>-0.812493</td>\n",
       "      <td>1.728397</td>\n",
       "      <td>3</td>\n",
       "      <td>page000_line011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.533533</td>\n",
       "      <td>-0.670471</td>\n",
       "      <td>0.355615</td>\n",
       "      <td>0.896869</td>\n",
       "      <td>-0.754374</td>\n",
       "      <td>-0.275247</td>\n",
       "      <td>-0.353749</td>\n",
       "      <td>-0.542393</td>\n",
       "      <td>1.092196</td>\n",
       "      <td>-0.064769</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.027692</td>\n",
       "      <td>0.260945</td>\n",
       "      <td>0.967058</td>\n",
       "      <td>0.834612</td>\n",
       "      <td>0.744693</td>\n",
       "      <td>1.368777</td>\n",
       "      <td>-0.804311</td>\n",
       "      <td>0.008971</td>\n",
       "      <td>3</td>\n",
       "      <td>page000_line004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.068758</td>\n",
       "      <td>0.346009</td>\n",
       "      <td>-0.542015</td>\n",
       "      <td>1.277306</td>\n",
       "      <td>-0.596175</td>\n",
       "      <td>-0.407558</td>\n",
       "      <td>-0.964712</td>\n",
       "      <td>-0.631917</td>\n",
       "      <td>0.919562</td>\n",
       "      <td>-0.919415</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.815364</td>\n",
       "      <td>1.021727</td>\n",
       "      <td>0.367840</td>\n",
       "      <td>1.065763</td>\n",
       "      <td>0.284883</td>\n",
       "      <td>1.136353</td>\n",
       "      <td>-0.157914</td>\n",
       "      <td>0.340986</td>\n",
       "      <td>3</td>\n",
       "      <td>page000_line010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.444731</td>\n",
       "      <td>0.605384</td>\n",
       "      <td>-0.279393</td>\n",
       "      <td>0.143534</td>\n",
       "      <td>0.397796</td>\n",
       "      <td>1.124081</td>\n",
       "      <td>-2.198709</td>\n",
       "      <td>-1.187628</td>\n",
       "      <td>-0.474654</td>\n",
       "      <td>-0.966479</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.146562</td>\n",
       "      <td>0.336720</td>\n",
       "      <td>0.784339</td>\n",
       "      <td>1.202369</td>\n",
       "      <td>-0.798215</td>\n",
       "      <td>1.212639</td>\n",
       "      <td>0.039807</td>\n",
       "      <td>0.586837</td>\n",
       "      <td>3</td>\n",
       "      <td>page000_line006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.397368</td>\n",
       "      <td>-0.024730</td>\n",
       "      <td>0.542317</td>\n",
       "      <td>0.416974</td>\n",
       "      <td>-0.672300</td>\n",
       "      <td>-0.102507</td>\n",
       "      <td>-1.310768</td>\n",
       "      <td>-1.528256</td>\n",
       "      <td>1.811763</td>\n",
       "      <td>-0.126442</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.013625</td>\n",
       "      <td>-0.614783</td>\n",
       "      <td>1.282822</td>\n",
       "      <td>0.886146</td>\n",
       "      <td>0.687368</td>\n",
       "      <td>-0.209796</td>\n",
       "      <td>-0.947312</td>\n",
       "      <td>1.506792</td>\n",
       "      <td>3</td>\n",
       "      <td>page000_line012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.548182</td>\n",
       "      <td>-0.110832</td>\n",
       "      <td>-0.298195</td>\n",
       "      <td>1.482604</td>\n",
       "      <td>0.476659</td>\n",
       "      <td>0.750116</td>\n",
       "      <td>-1.455401</td>\n",
       "      <td>-0.521254</td>\n",
       "      <td>-0.555038</td>\n",
       "      <td>-1.956491</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.474841</td>\n",
       "      <td>1.053458</td>\n",
       "      <td>1.735331</td>\n",
       "      <td>0.864692</td>\n",
       "      <td>-1.156795</td>\n",
       "      <td>1.644365</td>\n",
       "      <td>-0.368903</td>\n",
       "      <td>1.727635</td>\n",
       "      <td>3</td>\n",
       "      <td>page000_line007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.899175</td>\n",
       "      <td>-0.069753</td>\n",
       "      <td>0.300784</td>\n",
       "      <td>1.329214</td>\n",
       "      <td>0.137482</td>\n",
       "      <td>0.115308</td>\n",
       "      <td>-1.185902</td>\n",
       "      <td>0.520195</td>\n",
       "      <td>0.858706</td>\n",
       "      <td>-0.186683</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.164044</td>\n",
       "      <td>0.219707</td>\n",
       "      <td>0.772808</td>\n",
       "      <td>0.317163</td>\n",
       "      <td>0.655635</td>\n",
       "      <td>0.852677</td>\n",
       "      <td>-0.745911</td>\n",
       "      <td>2.183578</td>\n",
       "      <td>3</td>\n",
       "      <td>page000_line009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.284822</td>\n",
       "      <td>0.144479</td>\n",
       "      <td>1.242482</td>\n",
       "      <td>0.102223</td>\n",
       "      <td>0.683352</td>\n",
       "      <td>-0.399493</td>\n",
       "      <td>-2.077096</td>\n",
       "      <td>-0.811740</td>\n",
       "      <td>-0.200744</td>\n",
       "      <td>0.034794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042854</td>\n",
       "      <td>-0.243000</td>\n",
       "      <td>0.752491</td>\n",
       "      <td>0.674273</td>\n",
       "      <td>-1.771868</td>\n",
       "      <td>1.580923</td>\n",
       "      <td>-1.103293</td>\n",
       "      <td>0.902760</td>\n",
       "      <td>4</td>\n",
       "      <td>page000_line017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-1.417261</td>\n",
       "      <td>-0.195526</td>\n",
       "      <td>1.855916</td>\n",
       "      <td>-0.288126</td>\n",
       "      <td>-0.700317</td>\n",
       "      <td>0.378725</td>\n",
       "      <td>-1.840963</td>\n",
       "      <td>0.380007</td>\n",
       "      <td>0.541137</td>\n",
       "      <td>-1.394863</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.626281</td>\n",
       "      <td>-0.301799</td>\n",
       "      <td>1.171829</td>\n",
       "      <td>1.421664</td>\n",
       "      <td>-1.447193</td>\n",
       "      <td>1.612703</td>\n",
       "      <td>-0.498627</td>\n",
       "      <td>1.497717</td>\n",
       "      <td>4</td>\n",
       "      <td>page000_line002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.821136</td>\n",
       "      <td>-0.514354</td>\n",
       "      <td>1.380523</td>\n",
       "      <td>1.737670</td>\n",
       "      <td>0.180128</td>\n",
       "      <td>-0.046532</td>\n",
       "      <td>-1.691870</td>\n",
       "      <td>-1.303176</td>\n",
       "      <td>1.069632</td>\n",
       "      <td>-0.133193</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.534704</td>\n",
       "      <td>-0.607059</td>\n",
       "      <td>1.583575</td>\n",
       "      <td>0.309247</td>\n",
       "      <td>-0.134333</td>\n",
       "      <td>1.478819</td>\n",
       "      <td>-0.539329</td>\n",
       "      <td>2.961042</td>\n",
       "      <td>4</td>\n",
       "      <td>page000_line000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.760303</td>\n",
       "      <td>0.895791</td>\n",
       "      <td>1.491342</td>\n",
       "      <td>1.343078</td>\n",
       "      <td>-1.067421</td>\n",
       "      <td>-0.851370</td>\n",
       "      <td>-2.226929</td>\n",
       "      <td>-0.464046</td>\n",
       "      <td>0.225262</td>\n",
       "      <td>-1.644391</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.097913</td>\n",
       "      <td>-0.260226</td>\n",
       "      <td>0.071615</td>\n",
       "      <td>0.132198</td>\n",
       "      <td>-0.743547</td>\n",
       "      <td>2.387686</td>\n",
       "      <td>-2.363768</td>\n",
       "      <td>0.895487</td>\n",
       "      <td>4</td>\n",
       "      <td>page000_line015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.041007</td>\n",
       "      <td>1.633261</td>\n",
       "      <td>1.741746</td>\n",
       "      <td>0.625358</td>\n",
       "      <td>-2.341788</td>\n",
       "      <td>0.233289</td>\n",
       "      <td>-0.732800</td>\n",
       "      <td>0.654997</td>\n",
       "      <td>0.318321</td>\n",
       "      <td>-2.387836</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.583906</td>\n",
       "      <td>-1.380787</td>\n",
       "      <td>0.505839</td>\n",
       "      <td>1.876584</td>\n",
       "      <td>-1.684255</td>\n",
       "      <td>1.480990</td>\n",
       "      <td>-0.773467</td>\n",
       "      <td>0.373090</td>\n",
       "      <td>4</td>\n",
       "      <td>page000_line010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-1.236749</td>\n",
       "      <td>0.548504</td>\n",
       "      <td>1.412555</td>\n",
       "      <td>1.065041</td>\n",
       "      <td>-1.589259</td>\n",
       "      <td>1.144025</td>\n",
       "      <td>-1.113644</td>\n",
       "      <td>0.296600</td>\n",
       "      <td>0.547062</td>\n",
       "      <td>-1.320060</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.442539</td>\n",
       "      <td>-0.172453</td>\n",
       "      <td>0.856218</td>\n",
       "      <td>1.816082</td>\n",
       "      <td>-0.560511</td>\n",
       "      <td>2.593247</td>\n",
       "      <td>-0.520242</td>\n",
       "      <td>0.937336</td>\n",
       "      <td>4</td>\n",
       "      <td>page000_line007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-0.675129</td>\n",
       "      <td>-0.100372</td>\n",
       "      <td>0.126636</td>\n",
       "      <td>1.903031</td>\n",
       "      <td>-0.932906</td>\n",
       "      <td>-0.029992</td>\n",
       "      <td>-1.508575</td>\n",
       "      <td>-0.105283</td>\n",
       "      <td>0.430275</td>\n",
       "      <td>0.556498</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.010932</td>\n",
       "      <td>-0.733464</td>\n",
       "      <td>1.022323</td>\n",
       "      <td>0.607367</td>\n",
       "      <td>0.387326</td>\n",
       "      <td>-0.199046</td>\n",
       "      <td>-0.843895</td>\n",
       "      <td>1.147886</td>\n",
       "      <td>4</td>\n",
       "      <td>page000_line018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.822720</td>\n",
       "      <td>0.914817</td>\n",
       "      <td>1.206195</td>\n",
       "      <td>-0.679650</td>\n",
       "      <td>-0.894523</td>\n",
       "      <td>0.185949</td>\n",
       "      <td>-1.684727</td>\n",
       "      <td>0.270091</td>\n",
       "      <td>0.664255</td>\n",
       "      <td>-1.572916</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.540149</td>\n",
       "      <td>0.325710</td>\n",
       "      <td>0.972612</td>\n",
       "      <td>0.808860</td>\n",
       "      <td>-1.446238</td>\n",
       "      <td>2.323844</td>\n",
       "      <td>-1.479796</td>\n",
       "      <td>1.605714</td>\n",
       "      <td>4</td>\n",
       "      <td>page000_line019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>-0.458307</td>\n",
       "      <td>-0.958887</td>\n",
       "      <td>0.033891</td>\n",
       "      <td>0.733709</td>\n",
       "      <td>-1.697282</td>\n",
       "      <td>0.062691</td>\n",
       "      <td>-0.451608</td>\n",
       "      <td>-0.958972</td>\n",
       "      <td>0.942517</td>\n",
       "      <td>-0.618566</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.634955</td>\n",
       "      <td>0.610513</td>\n",
       "      <td>1.195423</td>\n",
       "      <td>-0.118988</td>\n",
       "      <td>0.832401</td>\n",
       "      <td>1.701328</td>\n",
       "      <td>-0.965071</td>\n",
       "      <td>0.606653</td>\n",
       "      <td>5</td>\n",
       "      <td>page000_line003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-0.445179</td>\n",
       "      <td>0.306864</td>\n",
       "      <td>0.132252</td>\n",
       "      <td>0.909049</td>\n",
       "      <td>0.891211</td>\n",
       "      <td>1.804006</td>\n",
       "      <td>-0.754798</td>\n",
       "      <td>-1.965100</td>\n",
       "      <td>0.157875</td>\n",
       "      <td>-0.436323</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.707230</td>\n",
       "      <td>1.010410</td>\n",
       "      <td>0.389437</td>\n",
       "      <td>0.302703</td>\n",
       "      <td>0.415911</td>\n",
       "      <td>2.397123</td>\n",
       "      <td>0.484518</td>\n",
       "      <td>0.141263</td>\n",
       "      <td>5</td>\n",
       "      <td>page000_line002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-0.252033</td>\n",
       "      <td>-0.476010</td>\n",
       "      <td>0.220876</td>\n",
       "      <td>0.839237</td>\n",
       "      <td>-1.177432</td>\n",
       "      <td>0.260475</td>\n",
       "      <td>-0.280225</td>\n",
       "      <td>-0.906387</td>\n",
       "      <td>0.332400</td>\n",
       "      <td>-1.462990</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.287458</td>\n",
       "      <td>1.036865</td>\n",
       "      <td>1.163799</td>\n",
       "      <td>-0.082156</td>\n",
       "      <td>-0.498177</td>\n",
       "      <td>2.336817</td>\n",
       "      <td>-0.725767</td>\n",
       "      <td>0.363254</td>\n",
       "      <td>5</td>\n",
       "      <td>page000_line000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-0.677300</td>\n",
       "      <td>-1.044457</td>\n",
       "      <td>-0.132667</td>\n",
       "      <td>0.260955</td>\n",
       "      <td>-1.494067</td>\n",
       "      <td>-0.060972</td>\n",
       "      <td>-0.560856</td>\n",
       "      <td>-0.633262</td>\n",
       "      <td>0.928054</td>\n",
       "      <td>-0.655661</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.441678</td>\n",
       "      <td>0.718351</td>\n",
       "      <td>1.001258</td>\n",
       "      <td>0.492759</td>\n",
       "      <td>0.786731</td>\n",
       "      <td>1.663528</td>\n",
       "      <td>-0.647642</td>\n",
       "      <td>0.667600</td>\n",
       "      <td>5</td>\n",
       "      <td>page000_line001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-0.309948</td>\n",
       "      <td>-0.487609</td>\n",
       "      <td>-0.666604</td>\n",
       "      <td>-0.209792</td>\n",
       "      <td>-0.239032</td>\n",
       "      <td>0.296499</td>\n",
       "      <td>-0.499096</td>\n",
       "      <td>-1.200930</td>\n",
       "      <td>0.313259</td>\n",
       "      <td>-0.516922</td>\n",
       "      <td>...</td>\n",
       "      <td>0.722910</td>\n",
       "      <td>0.663737</td>\n",
       "      <td>0.804323</td>\n",
       "      <td>0.604769</td>\n",
       "      <td>-0.081573</td>\n",
       "      <td>2.726238</td>\n",
       "      <td>-0.843379</td>\n",
       "      <td>-0.493157</td>\n",
       "      <td>5</td>\n",
       "      <td>page000_line005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-1.014642</td>\n",
       "      <td>-0.620240</td>\n",
       "      <td>-0.015325</td>\n",
       "      <td>-0.020156</td>\n",
       "      <td>-0.255930</td>\n",
       "      <td>0.195686</td>\n",
       "      <td>-0.450148</td>\n",
       "      <td>0.205388</td>\n",
       "      <td>0.008288</td>\n",
       "      <td>-0.246765</td>\n",
       "      <td>...</td>\n",
       "      <td>0.685066</td>\n",
       "      <td>0.696252</td>\n",
       "      <td>1.672011</td>\n",
       "      <td>-0.140407</td>\n",
       "      <td>-0.659373</td>\n",
       "      <td>1.700073</td>\n",
       "      <td>-0.392945</td>\n",
       "      <td>0.668212</td>\n",
       "      <td>5</td>\n",
       "      <td>page000_line004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.016063</td>\n",
       "      <td>-0.432868</td>\n",
       "      <td>0.193906</td>\n",
       "      <td>0.723087</td>\n",
       "      <td>-1.509290</td>\n",
       "      <td>0.137060</td>\n",
       "      <td>-0.258605</td>\n",
       "      <td>-0.686176</td>\n",
       "      <td>0.678195</td>\n",
       "      <td>-0.838486</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.594982</td>\n",
       "      <td>0.161270</td>\n",
       "      <td>1.042325</td>\n",
       "      <td>0.256729</td>\n",
       "      <td>0.359362</td>\n",
       "      <td>1.790579</td>\n",
       "      <td>-1.126071</td>\n",
       "      <td>0.215241</td>\n",
       "      <td>5</td>\n",
       "      <td>page000_line006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.930825</td>\n",
       "      <td>-0.975236</td>\n",
       "      <td>0.020694</td>\n",
       "      <td>-0.367364</td>\n",
       "      <td>-1.166840</td>\n",
       "      <td>0.080084</td>\n",
       "      <td>-0.683109</td>\n",
       "      <td>-0.879427</td>\n",
       "      <td>0.645276</td>\n",
       "      <td>-0.609716</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.961264</td>\n",
       "      <td>0.433916</td>\n",
       "      <td>1.019151</td>\n",
       "      <td>0.424877</td>\n",
       "      <td>0.402371</td>\n",
       "      <td>0.975544</td>\n",
       "      <td>-0.754261</td>\n",
       "      <td>0.599763</td>\n",
       "      <td>5</td>\n",
       "      <td>page000_line007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-0.381054</td>\n",
       "      <td>0.377363</td>\n",
       "      <td>-0.349268</td>\n",
       "      <td>1.053214</td>\n",
       "      <td>-0.778858</td>\n",
       "      <td>0.223258</td>\n",
       "      <td>0.133051</td>\n",
       "      <td>-0.763046</td>\n",
       "      <td>0.515453</td>\n",
       "      <td>-1.888904</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.357223</td>\n",
       "      <td>0.837642</td>\n",
       "      <td>1.390144</td>\n",
       "      <td>0.551295</td>\n",
       "      <td>-0.113882</td>\n",
       "      <td>2.524674</td>\n",
       "      <td>-0.827479</td>\n",
       "      <td>0.618074</td>\n",
       "      <td>5</td>\n",
       "      <td>page000_line009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-0.482022</td>\n",
       "      <td>-0.530190</td>\n",
       "      <td>0.555034</td>\n",
       "      <td>0.061592</td>\n",
       "      <td>-1.501323</td>\n",
       "      <td>0.552592</td>\n",
       "      <td>-0.453617</td>\n",
       "      <td>-0.881997</td>\n",
       "      <td>1.265358</td>\n",
       "      <td>-0.469314</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.759997</td>\n",
       "      <td>-0.130000</td>\n",
       "      <td>1.045778</td>\n",
       "      <td>0.133436</td>\n",
       "      <td>0.532937</td>\n",
       "      <td>1.433050</td>\n",
       "      <td>-0.866951</td>\n",
       "      <td>0.660781</td>\n",
       "      <td>5</td>\n",
       "      <td>page000_line008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-0.425163</td>\n",
       "      <td>-0.889256</td>\n",
       "      <td>0.680910</td>\n",
       "      <td>1.174501</td>\n",
       "      <td>-0.299338</td>\n",
       "      <td>1.439548</td>\n",
       "      <td>-1.017437</td>\n",
       "      <td>-2.315849</td>\n",
       "      <td>1.217635</td>\n",
       "      <td>-2.012322</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.548484</td>\n",
       "      <td>-0.967192</td>\n",
       "      <td>0.089741</td>\n",
       "      <td>0.390196</td>\n",
       "      <td>-0.447207</td>\n",
       "      <td>1.349188</td>\n",
       "      <td>-0.345251</td>\n",
       "      <td>-0.153885</td>\n",
       "      <td>6</td>\n",
       "      <td>page000_line003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-0.371337</td>\n",
       "      <td>-1.743062</td>\n",
       "      <td>0.156865</td>\n",
       "      <td>1.695572</td>\n",
       "      <td>-0.686044</td>\n",
       "      <td>0.506810</td>\n",
       "      <td>-1.419559</td>\n",
       "      <td>-2.573834</td>\n",
       "      <td>2.023610</td>\n",
       "      <td>-1.558440</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.066427</td>\n",
       "      <td>-1.449065</td>\n",
       "      <td>1.377814</td>\n",
       "      <td>0.322283</td>\n",
       "      <td>-1.037573</td>\n",
       "      <td>1.489391</td>\n",
       "      <td>-0.015439</td>\n",
       "      <td>0.281922</td>\n",
       "      <td>6</td>\n",
       "      <td>page000_line002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>-1.375505</td>\n",
       "      <td>-1.034224</td>\n",
       "      <td>0.728940</td>\n",
       "      <td>1.947940</td>\n",
       "      <td>-1.999517</td>\n",
       "      <td>0.405978</td>\n",
       "      <td>-1.565957</td>\n",
       "      <td>-2.758316</td>\n",
       "      <td>1.679069</td>\n",
       "      <td>-2.602715</td>\n",
       "      <td>...</td>\n",
       "      <td>0.782819</td>\n",
       "      <td>-1.618669</td>\n",
       "      <td>1.490047</td>\n",
       "      <td>-0.033443</td>\n",
       "      <td>-1.110051</td>\n",
       "      <td>2.042918</td>\n",
       "      <td>-1.068701</td>\n",
       "      <td>0.377701</td>\n",
       "      <td>6</td>\n",
       "      <td>page000_line000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>-1.546735</td>\n",
       "      <td>-0.665076</td>\n",
       "      <td>0.014008</td>\n",
       "      <td>2.013012</td>\n",
       "      <td>-0.570232</td>\n",
       "      <td>0.899402</td>\n",
       "      <td>-0.864737</td>\n",
       "      <td>-2.181496</td>\n",
       "      <td>1.139154</td>\n",
       "      <td>-2.252067</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.292332</td>\n",
       "      <td>-0.215581</td>\n",
       "      <td>0.287592</td>\n",
       "      <td>0.188674</td>\n",
       "      <td>-0.013113</td>\n",
       "      <td>2.173856</td>\n",
       "      <td>-0.742428</td>\n",
       "      <td>-0.962788</td>\n",
       "      <td>6</td>\n",
       "      <td>page000_line001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47 rows × 642 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6  \\\n",
       "0   0.900881 -1.535855  1.216326  0.263924  0.861893  0.184795 -1.468211   \n",
       "1   1.134265 -1.362282  2.273987  0.284600 -0.652768 -0.222237 -0.832183   \n",
       "2   0.593472 -0.976235  0.744769  0.308163 -0.685127 -0.755348 -2.012943   \n",
       "3   1.280957 -1.285501  1.829224  0.499230 -0.052572  0.709289 -1.074080   \n",
       "4   1.312774 -0.413435  2.063291  0.887190  0.801703 -0.023718 -2.104139   \n",
       "5  -0.088423 -0.558079  2.144183  0.348551 -0.823501 -0.331861 -1.095646   \n",
       "6   1.352457 -1.241885  1.946865  0.645203 -0.915543 -0.268871 -1.386410   \n",
       "7   1.507467 -1.915475  2.334567  0.967679  0.755115  0.768464 -1.766531   \n",
       "8   1.369867 -1.144850  2.374497  1.486444  0.057034  0.659323 -0.906150   \n",
       "9  -0.611749 -2.095418  0.496747  1.899151 -0.842998  0.447551 -1.261050   \n",
       "10 -2.290432 -1.823981 -0.245025  1.480978 -0.356717  0.568426 -0.646507   \n",
       "11  0.207571 -1.161908 -0.629806  1.927256  0.069100  0.012873 -0.593543   \n",
       "12 -1.589233 -1.054851 -0.275232  2.352102 -1.657410  0.068928 -0.326524   \n",
       "13 -0.566000 -1.090448  0.432487  1.193386 -0.284967  0.979698 -1.883320   \n",
       "14 -0.757621 -0.894421  0.013984  2.179670 -1.608992  0.379539 -1.496733   \n",
       "15 -1.511285 -0.159393 -1.285892  0.883206 -0.240112 -0.082843 -1.698460   \n",
       "16 -0.915069  0.126906 -1.253779  1.961727 -0.480075  0.358586 -1.083527   \n",
       "17 -1.399246 -0.449594 -0.050102  0.851918 -0.061686  0.489559 -1.947546   \n",
       "18 -0.400874 -0.573764  0.262399  1.218095 -1.354946 -0.158789 -1.226880   \n",
       "19 -0.533533 -0.670471  0.355615  0.896869 -0.754374 -0.275247 -0.353749   \n",
       "20  0.068758  0.346009 -0.542015  1.277306 -0.596175 -0.407558 -0.964712   \n",
       "21 -0.444731  0.605384 -0.279393  0.143534  0.397796  1.124081 -2.198709   \n",
       "22 -0.397368 -0.024730  0.542317  0.416974 -0.672300 -0.102507 -1.310768   \n",
       "23  0.548182 -0.110832 -0.298195  1.482604  0.476659  0.750116 -1.455401   \n",
       "24 -0.899175 -0.069753  0.300784  1.329214  0.137482  0.115308 -1.185902   \n",
       "25  0.284822  0.144479  1.242482  0.102223  0.683352 -0.399493 -2.077096   \n",
       "26 -1.417261 -0.195526  1.855916 -0.288126 -0.700317  0.378725 -1.840963   \n",
       "27  0.821136 -0.514354  1.380523  1.737670  0.180128 -0.046532 -1.691870   \n",
       "28  0.760303  0.895791  1.491342  1.343078 -1.067421 -0.851370 -2.226929   \n",
       "29 -0.041007  1.633261  1.741746  0.625358 -2.341788  0.233289 -0.732800   \n",
       "30 -1.236749  0.548504  1.412555  1.065041 -1.589259  1.144025 -1.113644   \n",
       "31 -0.675129 -0.100372  0.126636  1.903031 -0.932906 -0.029992 -1.508575   \n",
       "32  0.822720  0.914817  1.206195 -0.679650 -0.894523  0.185949 -1.684727   \n",
       "33 -0.458307 -0.958887  0.033891  0.733709 -1.697282  0.062691 -0.451608   \n",
       "34 -0.445179  0.306864  0.132252  0.909049  0.891211  1.804006 -0.754798   \n",
       "35 -0.252033 -0.476010  0.220876  0.839237 -1.177432  0.260475 -0.280225   \n",
       "36 -0.677300 -1.044457 -0.132667  0.260955 -1.494067 -0.060972 -0.560856   \n",
       "37 -0.309948 -0.487609 -0.666604 -0.209792 -0.239032  0.296499 -0.499096   \n",
       "38 -1.014642 -0.620240 -0.015325 -0.020156 -0.255930  0.195686 -0.450148   \n",
       "39  0.016063 -0.432868  0.193906  0.723087 -1.509290  0.137060 -0.258605   \n",
       "40 -0.930825 -0.975236  0.020694 -0.367364 -1.166840  0.080084 -0.683109   \n",
       "41 -0.381054  0.377363 -0.349268  1.053214 -0.778858  0.223258  0.133051   \n",
       "42 -0.482022 -0.530190  0.555034  0.061592 -1.501323  0.552592 -0.453617   \n",
       "43 -0.425163 -0.889256  0.680910  1.174501 -0.299338  1.439548 -1.017437   \n",
       "44 -0.371337 -1.743062  0.156865  1.695572 -0.686044  0.506810 -1.419559   \n",
       "45 -1.375505 -1.034224  0.728940  1.947940 -1.999517  0.405978 -1.565957   \n",
       "46 -1.546735 -0.665076  0.014008  2.013012 -0.570232  0.899402 -0.864737   \n",
       "\n",
       "           7         8         9  ...       632       633       634       635  \\\n",
       "0  -0.903002  0.471242 -1.091495  ...  0.058538 -1.304109  1.541442 -0.424794   \n",
       "1  -1.145109  0.296855 -2.368747  ...  0.157107 -1.092878  1.892547 -0.487107   \n",
       "2   0.673236 -0.896982 -1.988740  ... -0.753516 -0.608883  1.339883  0.432048   \n",
       "3  -1.166486  0.981797 -1.591591  ...  0.361287 -0.495855  1.635529 -0.772384   \n",
       "4   0.721972  0.649561 -2.637098  ... -0.471062  0.093955  0.671852  0.130571   \n",
       "5   0.077632  0.161654 -2.837906  ... -0.645933 -0.706350  0.919395  1.023801   \n",
       "6  -0.908995  0.262660 -2.861198  ...  0.164378  0.277399  2.058115  0.161064   \n",
       "7  -0.953910  0.261470 -2.457000  ...  0.438582 -0.710739  1.944403 -0.137118   \n",
       "8  -0.305979 -0.087208 -2.945641  ...  0.621893 -1.044549  1.939120  0.076557   \n",
       "9  -2.167131  1.413064 -2.242526  ...  0.244677 -0.928682  1.154003 -0.196702   \n",
       "10 -2.733148  2.006800 -2.515923  ...  0.518083  0.408829  0.237417  0.417480   \n",
       "11 -1.862664  1.482231 -1.305336  ... -0.328434 -0.731346  0.970486  1.150844   \n",
       "12 -2.286921  1.303765 -1.600224  ...  0.040765 -0.186851  1.600801 -1.122975   \n",
       "13 -2.088283  1.133648 -1.532070  ... -0.282095 -0.349165  0.040628  0.709523   \n",
       "14 -1.766940  1.191479 -2.852113  ...  0.270399 -1.446855  0.904377  0.191518   \n",
       "15 -0.617277  0.631346 -0.851741  ... -1.025800 -0.099167  0.538432  1.420944   \n",
       "16 -0.397470  0.726015 -0.928387  ... -0.101263  1.035422  1.443239  1.378848   \n",
       "17 -0.186119  0.788533 -0.059928  ... -0.674960 -0.008328  1.219678  0.704170   \n",
       "18 -1.198591  1.332944 -0.620245  ... -1.091506 -0.495805  1.265438 -0.169847   \n",
       "19 -0.542393  1.092196 -0.064769  ... -1.027692  0.260945  0.967058  0.834612   \n",
       "20 -0.631917  0.919562 -0.919415  ... -0.815364  1.021727  0.367840  1.065763   \n",
       "21 -1.187628 -0.474654 -0.966479  ... -1.146562  0.336720  0.784339  1.202369   \n",
       "22 -1.528256  1.811763 -0.126442  ... -1.013625 -0.614783  1.282822  0.886146   \n",
       "23 -0.521254 -0.555038 -1.956491  ... -0.474841  1.053458  1.735331  0.864692   \n",
       "24  0.520195  0.858706 -0.186683  ... -1.164044  0.219707  0.772808  0.317163   \n",
       "25 -0.811740 -0.200744  0.034794  ...  0.042854 -0.243000  0.752491  0.674273   \n",
       "26  0.380007  0.541137 -1.394863  ... -0.626281 -0.301799  1.171829  1.421664   \n",
       "27 -1.303176  1.069632 -0.133193  ... -0.534704 -0.607059  1.583575  0.309247   \n",
       "28 -0.464046  0.225262 -1.644391  ... -0.097913 -0.260226  0.071615  0.132198   \n",
       "29  0.654997  0.318321 -2.387836  ... -0.583906 -1.380787  0.505839  1.876584   \n",
       "30  0.296600  0.547062 -1.320060  ... -0.442539 -0.172453  0.856218  1.816082   \n",
       "31 -0.105283  0.430275  0.556498  ... -1.010932 -0.733464  1.022323  0.607367   \n",
       "32  0.270091  0.664255 -1.572916  ... -0.540149  0.325710  0.972612  0.808860   \n",
       "33 -0.958972  0.942517 -0.618566  ... -0.634955  0.610513  1.195423 -0.118988   \n",
       "34 -1.965100  0.157875 -0.436323  ... -0.707230  1.010410  0.389437  0.302703   \n",
       "35 -0.906387  0.332400 -1.462990  ... -0.287458  1.036865  1.163799 -0.082156   \n",
       "36 -0.633262  0.928054 -0.655661  ... -0.441678  0.718351  1.001258  0.492759   \n",
       "37 -1.200930  0.313259 -0.516922  ...  0.722910  0.663737  0.804323  0.604769   \n",
       "38  0.205388  0.008288 -0.246765  ...  0.685066  0.696252  1.672011 -0.140407   \n",
       "39 -0.686176  0.678195 -0.838486  ... -0.594982  0.161270  1.042325  0.256729   \n",
       "40 -0.879427  0.645276 -0.609716  ... -0.961264  0.433916  1.019151  0.424877   \n",
       "41 -0.763046  0.515453 -1.888904  ... -0.357223  0.837642  1.390144  0.551295   \n",
       "42 -0.881997  1.265358 -0.469314  ... -0.759997 -0.130000  1.045778  0.133436   \n",
       "43 -2.315849  1.217635 -2.012322  ... -0.548484 -0.967192  0.089741  0.390196   \n",
       "44 -2.573834  2.023610 -1.558440  ... -0.066427 -1.449065  1.377814  0.322283   \n",
       "45 -2.758316  1.679069 -2.602715  ...  0.782819 -1.618669  1.490047 -0.033443   \n",
       "46 -2.181496  1.139154 -2.252067  ... -0.292332 -0.215581  0.287592  0.188674   \n",
       "\n",
       "         636       637       638       639  name             form  \n",
       "0  -0.802706  1.041371 -0.328758  3.136823     0  page000_line003  \n",
       "1   0.093969  2.060857 -1.213497  2.975895     0  page000_line002  \n",
       "2   0.636367  1.714106  0.504608  1.266553     0  page000_line000  \n",
       "3   1.012281  2.350589 -0.735705  2.450419     0  page000_line001  \n",
       "4  -0.747895  2.493425 -1.339399  1.875247     0  page000_line005  \n",
       "5  -0.501715  1.016948 -1.168648  1.912641     0  page000_line004  \n",
       "6   0.208770  3.003321 -1.318584  2.281312     0  page000_line006  \n",
       "7  -0.349195  2.496340 -0.891258  2.711566     0  page000_line007  \n",
       "8   0.389761  1.469566 -0.691275  1.780019     0  page000_line008  \n",
       "9  -1.030584  1.451702 -0.573324  0.096913     1  page000_line003  \n",
       "10  0.410241  1.516244 -0.792029 -1.057558     1  page000_line002  \n",
       "11 -0.227760  0.252524  0.596436 -0.253897     1  page000_line000  \n",
       "12 -0.408834  3.436998 -1.169829  0.025704     1  page000_line001  \n",
       "13 -0.784128  1.829252 -1.324068  0.003215     1  page000_line005  \n",
       "14 -1.545192  2.216811 -1.141569 -1.106837     1  page000_line004  \n",
       "15 -0.220729  1.039717 -0.968723  1.182505     3  page000_line003  \n",
       "16  0.203002  2.551162 -1.204188  1.095948     3  page000_line002  \n",
       "17  0.378878  1.588672 -0.762329  1.949395     3  page000_line001  \n",
       "18  1.055912  0.814413 -0.812493  1.728397     3  page000_line011  \n",
       "19  0.744693  1.368777 -0.804311  0.008971     3  page000_line004  \n",
       "20  0.284883  1.136353 -0.157914  0.340986     3  page000_line010  \n",
       "21 -0.798215  1.212639  0.039807  0.586837     3  page000_line006  \n",
       "22  0.687368 -0.209796 -0.947312  1.506792     3  page000_line012  \n",
       "23 -1.156795  1.644365 -0.368903  1.727635     3  page000_line007  \n",
       "24  0.655635  0.852677 -0.745911  2.183578     3  page000_line009  \n",
       "25 -1.771868  1.580923 -1.103293  0.902760     4  page000_line017  \n",
       "26 -1.447193  1.612703 -0.498627  1.497717     4  page000_line002  \n",
       "27 -0.134333  1.478819 -0.539329  2.961042     4  page000_line000  \n",
       "28 -0.743547  2.387686 -2.363768  0.895487     4  page000_line015  \n",
       "29 -1.684255  1.480990 -0.773467  0.373090     4  page000_line010  \n",
       "30 -0.560511  2.593247 -0.520242  0.937336     4  page000_line007  \n",
       "31  0.387326 -0.199046 -0.843895  1.147886     4  page000_line018  \n",
       "32 -1.446238  2.323844 -1.479796  1.605714     4  page000_line019  \n",
       "33  0.832401  1.701328 -0.965071  0.606653     5  page000_line003  \n",
       "34  0.415911  2.397123  0.484518  0.141263     5  page000_line002  \n",
       "35 -0.498177  2.336817 -0.725767  0.363254     5  page000_line000  \n",
       "36  0.786731  1.663528 -0.647642  0.667600     5  page000_line001  \n",
       "37 -0.081573  2.726238 -0.843379 -0.493157     5  page000_line005  \n",
       "38 -0.659373  1.700073 -0.392945  0.668212     5  page000_line004  \n",
       "39  0.359362  1.790579 -1.126071  0.215241     5  page000_line006  \n",
       "40  0.402371  0.975544 -0.754261  0.599763     5  page000_line007  \n",
       "41 -0.113882  2.524674 -0.827479  0.618074     5  page000_line009  \n",
       "42  0.532937  1.433050 -0.866951  0.660781     5  page000_line008  \n",
       "43 -0.447207  1.349188 -0.345251 -0.153885     6  page000_line003  \n",
       "44 -1.037573  1.489391 -0.015439  0.281922     6  page000_line002  \n",
       "45 -1.110051  2.042918 -1.068701  0.377701     6  page000_line000  \n",
       "46 -0.013113  2.173856 -0.742428 -0.962788     6  page000_line001  \n",
       "\n",
       "[47 rows x 642 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features = feature_extraction(real_world_data['name'], real_world_data['images_path'])\n",
    "all_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d640991",
   "metadata": {},
   "source": [
    "# Feature Extraction for Training Set (class-wise fixed sizes and bootstrapping)\n",
    "- 3/4 Sentence-Level Images per Sub-writer Profile, for 6 Sub-writer Profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec8a8a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sub_writers_bootstrap(train_df, feature_store, images_per_subwriter=4, num_profiles_per_writer=6, fold_seed=42):\n",
    "    writer_names_in_fold = train_df['name'].unique()\n",
    "    feature_df = feature_store[feature_store['name'].isin(writer_names_in_fold)]\n",
    "\n",
    "    feature_columns = [col for col in feature_df.columns if col not in ['name', 'form']]\n",
    "    aggregated_data = []\n",
    "    \n",
    "    unique_writers = feature_df['name'].unique()\n",
    "    \n",
    "    for writer_id in unique_writers:\n",
    "        writer_df = feature_df[feature_df['name'] == writer_id]\n",
    "        num_images = len(writer_df)\n",
    "        \n",
    "        if num_images == 0:\n",
    "            continue\n",
    "\n",
    "        if num_images <= images_per_subwriter:\n",
    "            \n",
    "            feature_chunk = writer_df[feature_columns]\n",
    "            mean_features = feature_chunk.mean(axis=0)\n",
    "            median_features = feature_chunk.median(axis=0)\n",
    "            std_features = feature_chunk.std(axis=0).fillna(0)\n",
    "            skew_features = feature_chunk.skew(axis=0).fillna(0)\n",
    "            \n",
    "            new_row = {}\n",
    "            for col in feature_columns:\n",
    "                new_row[f'{col}|mean'] = mean_features[col]\n",
    "                new_row[f'{col}|median'] = median_features[col]\n",
    "                new_row[f'{col}|std'] = std_features[col]\n",
    "                new_row[f'{col}|skew'] = skew_features[col]\n",
    "                \n",
    "            \n",
    "            new_row['original_writer_id'] = writer_id\n",
    "            new_row['sub_writer_id'] = f\"{writer_id}-agg-0\"\n",
    "            aggregated_data.append(new_row)\n",
    "            continue\n",
    "\n",
    "        writer_profiles = []\n",
    "        \n",
    "        max_attempts = num_profiles_per_writer * 5\n",
    "        \n",
    "        for i in range(max_attempts):\n",
    "            sample_chunk = writer_df.sample(n=images_per_subwriter, replace=True, random_state=fold_seed + hash(writer_id) % 100000 + i)\n",
    "            feature_chunk = sample_chunk[feature_columns]\n",
    "            \n",
    "            mean_features = feature_chunk.mean(axis=0)\n",
    "            median_features = feature_chunk.median(axis=0)\n",
    "            std_features = feature_chunk.std(axis=0).fillna(0)\n",
    "            skew_features = feature_chunk.skew(axis=0).fillna(0)\n",
    "            \n",
    "            profile_dict = {}\n",
    "            for col in feature_columns:\n",
    "                profile_dict[f'{col}|mean'] = mean_features[col]\n",
    "                profile_dict[f'{col}|median'] = median_features[col]\n",
    "                profile_dict[f'{col}|std'] = std_features[col]\n",
    "                profile_dict[f'{col}|skew'] = skew_features[col]\n",
    "                \n",
    "            writer_profiles.append(profile_dict)\n",
    "            \n",
    "            temp_df = pd.DataFrame(writer_profiles).drop_duplicates()\n",
    "            if len(temp_df) >= num_profiles_per_writer:\n",
    "                break\n",
    "        \n",
    "        if writer_profiles:\n",
    "            profiles_df = pd.DataFrame(writer_profiles)\n",
    "            unique_profiles_df = profiles_df.drop_duplicates().reset_index(drop=True)\n",
    "            final_profiles_df = unique_profiles_df.head(num_profiles_per_writer)\n",
    "\n",
    "            for i, row in final_profiles_df.iterrows():\n",
    "                profile_data = row.to_dict()\n",
    "                profile_data['original_writer_id'] = writer_id\n",
    "                profile_data['sub_writer_id'] = f\"{writer_id}-boot-{i}\"\n",
    "                aggregated_data.append(profile_data)\n",
    "            \n",
    "    \n",
    "    final_df = pd.DataFrame(aggregated_data)\n",
    "    \n",
    "    new_feature_columns = []\n",
    "    for col in feature_columns:\n",
    "        new_feature_columns.append(f'{col}|mean')\n",
    "        new_feature_columns.append(f'{col}|median')\n",
    "        new_feature_columns.append(f'{col}|std')\n",
    "        new_feature_columns.append(f'{col}|skew')\n",
    "\n",
    "    cols = ['sub_writer_id', 'original_writer_id'] + new_feature_columns\n",
    "    final_df = final_df[cols]\n",
    "    \n",
    "    final_df = pd.merge(final_df, train_df[['name', 'NativeLanguage']], how='inner', left_on='original_writer_id', right_on='name')\n",
    "    \n",
    "    X_train = final_df.drop(columns=['sub_writer_id', 'original_writer_id','name','NativeLanguage'])\n",
    "    y_train = final_df['NativeLanguage']\n",
    "    y_train = np.vectorize(convert_y)(y_train)\n",
    "    \n",
    "    return X_train, y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17cf4667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>632</th>\n",
       "      <th>633</th>\n",
       "      <th>634</th>\n",
       "      <th>635</th>\n",
       "      <th>636</th>\n",
       "      <th>637</th>\n",
       "      <th>638</th>\n",
       "      <th>639</th>\n",
       "      <th>name</th>\n",
       "      <th>form</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.372276</td>\n",
       "      <td>0.224989</td>\n",
       "      <td>-0.467460</td>\n",
       "      <td>1.820759</td>\n",
       "      <td>-0.586967</td>\n",
       "      <td>1.776678</td>\n",
       "      <td>-3.153311</td>\n",
       "      <td>-2.495186</td>\n",
       "      <td>0.968704</td>\n",
       "      <td>-1.006844</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.981443</td>\n",
       "      <td>-0.273160</td>\n",
       "      <td>1.014988</td>\n",
       "      <td>1.218125</td>\n",
       "      <td>-0.981978</td>\n",
       "      <td>3.083691</td>\n",
       "      <td>-1.024849</td>\n",
       "      <td>2.180169</td>\n",
       "      <td>10000</td>\n",
       "      <td>z01-000-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.033635</td>\n",
       "      <td>-0.905358</td>\n",
       "      <td>0.258783</td>\n",
       "      <td>1.923517</td>\n",
       "      <td>-0.866238</td>\n",
       "      <td>0.205671</td>\n",
       "      <td>-1.162825</td>\n",
       "      <td>-0.941368</td>\n",
       "      <td>1.491512</td>\n",
       "      <td>-0.259629</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.438199</td>\n",
       "      <td>-1.047727</td>\n",
       "      <td>1.526454</td>\n",
       "      <td>-0.711613</td>\n",
       "      <td>-0.884340</td>\n",
       "      <td>0.576500</td>\n",
       "      <td>-1.261368</td>\n",
       "      <td>1.769279</td>\n",
       "      <td>10000</td>\n",
       "      <td>z01-000-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.025888</td>\n",
       "      <td>-0.592650</td>\n",
       "      <td>0.585571</td>\n",
       "      <td>2.040271</td>\n",
       "      <td>-0.155405</td>\n",
       "      <td>-0.120354</td>\n",
       "      <td>-1.829877</td>\n",
       "      <td>-1.252309</td>\n",
       "      <td>1.205214</td>\n",
       "      <td>-0.259600</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.126850</td>\n",
       "      <td>-1.122187</td>\n",
       "      <td>0.977126</td>\n",
       "      <td>0.345049</td>\n",
       "      <td>-0.814934</td>\n",
       "      <td>0.962746</td>\n",
       "      <td>-0.548778</td>\n",
       "      <td>2.673030</td>\n",
       "      <td>10000</td>\n",
       "      <td>z01-000-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.510987</td>\n",
       "      <td>-0.666809</td>\n",
       "      <td>-1.368710</td>\n",
       "      <td>1.987004</td>\n",
       "      <td>-0.221620</td>\n",
       "      <td>0.672618</td>\n",
       "      <td>-3.057467</td>\n",
       "      <td>-1.380301</td>\n",
       "      <td>1.458408</td>\n",
       "      <td>-1.007992</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.292161</td>\n",
       "      <td>-1.257141</td>\n",
       "      <td>-0.041370</td>\n",
       "      <td>1.655186</td>\n",
       "      <td>-0.791135</td>\n",
       "      <td>2.099786</td>\n",
       "      <td>-1.133589</td>\n",
       "      <td>1.716038</td>\n",
       "      <td>10000</td>\n",
       "      <td>z01-000-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.917990</td>\n",
       "      <td>-0.359039</td>\n",
       "      <td>0.085779</td>\n",
       "      <td>1.985653</td>\n",
       "      <td>-1.203340</td>\n",
       "      <td>0.940839</td>\n",
       "      <td>-2.236159</td>\n",
       "      <td>-2.180054</td>\n",
       "      <td>1.759380</td>\n",
       "      <td>-1.052856</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.828048</td>\n",
       "      <td>-1.400792</td>\n",
       "      <td>1.134104</td>\n",
       "      <td>0.957654</td>\n",
       "      <td>-0.926594</td>\n",
       "      <td>2.193790</td>\n",
       "      <td>-1.190055</td>\n",
       "      <td>2.018496</td>\n",
       "      <td>10000</td>\n",
       "      <td>z01-000-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2836</th>\n",
       "      <td>-0.166905</td>\n",
       "      <td>-1.087335</td>\n",
       "      <td>0.436946</td>\n",
       "      <td>0.041008</td>\n",
       "      <td>-1.445201</td>\n",
       "      <td>0.030634</td>\n",
       "      <td>-0.942336</td>\n",
       "      <td>-1.672251</td>\n",
       "      <td>2.018108</td>\n",
       "      <td>0.040737</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.728344</td>\n",
       "      <td>-0.209595</td>\n",
       "      <td>1.440409</td>\n",
       "      <td>0.387701</td>\n",
       "      <td>0.704579</td>\n",
       "      <td>0.775189</td>\n",
       "      <td>-1.445656</td>\n",
       "      <td>1.703038</td>\n",
       "      <td>25</td>\n",
       "      <td>page000_line001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2837</th>\n",
       "      <td>-1.020958</td>\n",
       "      <td>-0.973095</td>\n",
       "      <td>-0.202618</td>\n",
       "      <td>0.460286</td>\n",
       "      <td>-0.908080</td>\n",
       "      <td>0.431273</td>\n",
       "      <td>-1.310988</td>\n",
       "      <td>-2.186874</td>\n",
       "      <td>1.921045</td>\n",
       "      <td>-1.022282</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.463473</td>\n",
       "      <td>-0.259575</td>\n",
       "      <td>0.989406</td>\n",
       "      <td>0.643102</td>\n",
       "      <td>0.046823</td>\n",
       "      <td>0.620898</td>\n",
       "      <td>-0.797454</td>\n",
       "      <td>2.097991</td>\n",
       "      <td>25</td>\n",
       "      <td>page000_line005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2838</th>\n",
       "      <td>0.187808</td>\n",
       "      <td>-0.616186</td>\n",
       "      <td>0.712529</td>\n",
       "      <td>1.516693</td>\n",
       "      <td>-1.776467</td>\n",
       "      <td>1.043272</td>\n",
       "      <td>-0.366190</td>\n",
       "      <td>-1.701217</td>\n",
       "      <td>1.274346</td>\n",
       "      <td>-1.977084</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.142874</td>\n",
       "      <td>1.226076</td>\n",
       "      <td>0.536600</td>\n",
       "      <td>0.963856</td>\n",
       "      <td>0.226091</td>\n",
       "      <td>1.882359</td>\n",
       "      <td>-2.185912</td>\n",
       "      <td>1.138407</td>\n",
       "      <td>25</td>\n",
       "      <td>page000_line006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2839</th>\n",
       "      <td>-0.546677</td>\n",
       "      <td>-1.329427</td>\n",
       "      <td>-0.422880</td>\n",
       "      <td>1.233715</td>\n",
       "      <td>-1.006450</td>\n",
       "      <td>0.287910</td>\n",
       "      <td>-1.033419</td>\n",
       "      <td>-2.418210</td>\n",
       "      <td>1.775813</td>\n",
       "      <td>-1.206054</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.083004</td>\n",
       "      <td>-0.112003</td>\n",
       "      <td>0.532118</td>\n",
       "      <td>0.783740</td>\n",
       "      <td>0.341568</td>\n",
       "      <td>0.286427</td>\n",
       "      <td>-0.865030</td>\n",
       "      <td>1.536021</td>\n",
       "      <td>25</td>\n",
       "      <td>page000_line009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2840</th>\n",
       "      <td>-1.812275</td>\n",
       "      <td>-0.585898</td>\n",
       "      <td>0.796219</td>\n",
       "      <td>1.554841</td>\n",
       "      <td>-0.928513</td>\n",
       "      <td>0.570940</td>\n",
       "      <td>-1.218171</td>\n",
       "      <td>-2.051419</td>\n",
       "      <td>1.197945</td>\n",
       "      <td>-1.020835</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.561254</td>\n",
       "      <td>0.868786</td>\n",
       "      <td>-0.505556</td>\n",
       "      <td>1.034004</td>\n",
       "      <td>0.054721</td>\n",
       "      <td>0.630044</td>\n",
       "      <td>-1.163606</td>\n",
       "      <td>-0.114015</td>\n",
       "      <td>25</td>\n",
       "      <td>page000_line008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2841 rows × 642 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0    -0.372276  0.224989 -0.467460  1.820759 -0.586967  1.776678 -3.153311   \n",
       "1    -0.033635 -0.905358  0.258783  1.923517 -0.866238  0.205671 -1.162825   \n",
       "2     0.025888 -0.592650  0.585571  2.040271 -0.155405 -0.120354 -1.829877   \n",
       "3    -1.510987 -0.666809 -1.368710  1.987004 -0.221620  0.672618 -3.057467   \n",
       "4    -0.917990 -0.359039  0.085779  1.985653 -1.203340  0.940839 -2.236159   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2836 -0.166905 -1.087335  0.436946  0.041008 -1.445201  0.030634 -0.942336   \n",
       "2837 -1.020958 -0.973095 -0.202618  0.460286 -0.908080  0.431273 -1.310988   \n",
       "2838  0.187808 -0.616186  0.712529  1.516693 -1.776467  1.043272 -0.366190   \n",
       "2839 -0.546677 -1.329427 -0.422880  1.233715 -1.006450  0.287910 -1.033419   \n",
       "2840 -1.812275 -0.585898  0.796219  1.554841 -0.928513  0.570940 -1.218171   \n",
       "\n",
       "             7         8         9  ...       632       633       634  \\\n",
       "0    -2.495186  0.968704 -1.006844  ... -0.981443 -0.273160  1.014988   \n",
       "1    -0.941368  1.491512 -0.259629  ... -0.438199 -1.047727  1.526454   \n",
       "2    -1.252309  1.205214 -0.259600  ... -1.126850 -1.122187  0.977126   \n",
       "3    -1.380301  1.458408 -1.007992  ... -1.292161 -1.257141 -0.041370   \n",
       "4    -2.180054  1.759380 -1.052856  ... -0.828048 -1.400792  1.134104   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "2836 -1.672251  2.018108  0.040737  ... -1.728344 -0.209595  1.440409   \n",
       "2837 -2.186874  1.921045 -1.022282  ... -1.463473 -0.259575  0.989406   \n",
       "2838 -1.701217  1.274346 -1.977084  ... -0.142874  1.226076  0.536600   \n",
       "2839 -2.418210  1.775813 -1.206054  ... -1.083004 -0.112003  0.532118   \n",
       "2840 -2.051419  1.197945 -1.020835  ... -2.561254  0.868786 -0.505556   \n",
       "\n",
       "           635       636       637       638       639   name             form  \n",
       "0     1.218125 -0.981978  3.083691 -1.024849  2.180169  10000       z01-000-03  \n",
       "1    -0.711613 -0.884340  0.576500 -1.261368  1.769279  10000       z01-000-02  \n",
       "2     0.345049 -0.814934  0.962746 -0.548778  2.673030  10000       z01-000-01  \n",
       "3     1.655186 -0.791135  2.099786 -1.133589  1.716038  10000       z01-000-05  \n",
       "4     0.957654 -0.926594  2.193790 -1.190055  2.018496  10000       z01-000-04  \n",
       "...        ...       ...       ...       ...       ...    ...              ...  \n",
       "2836  0.387701  0.704579  0.775189 -1.445656  1.703038     25  page000_line001  \n",
       "2837  0.643102  0.046823  0.620898 -0.797454  2.097991     25  page000_line005  \n",
       "2838  0.963856  0.226091  1.882359 -2.185912  1.138407     25  page000_line006  \n",
       "2839  0.783740  0.341568  0.286427 -0.865030  1.536021     25  page000_line009  \n",
       "2840  1.034004  0.054721  0.630044 -1.163606 -0.114015     25  page000_line008  \n",
       "\n",
       "[2841 rows x 642 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_store = pd.read_csv('./Final-Data/All-Writers-Feature-Vectors-GrayScale.csv', index_col=[0])\n",
    "display(feature_store)\n",
    "writers_info = pd.read_csv('./Final-Data/new-writers-info.csv')\n",
    "english_writer_info = pd.read_csv('../Data/Bristol-Corpus/English-GrayScale/english_df.csv')\n",
    "writers_info = pd.concat([writers_info, english_writer_info], axis=0, ignore_index=True)\n",
    "writers_info = writers_info.loc[(writers_info['NativeLanguage'] != 'Swiss German')]\n",
    "all_writers_info = feature_store[['name']].drop_duplicates().reset_index(drop=True)\n",
    "all_writers_info = pd.merge(all_writers_info, writers_info[['name', 'NativeLanguage']], how='inner', on='name')\n",
    "all_labels = all_writers_info['NativeLanguage']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d6368a",
   "metadata": {},
   "source": [
    "## X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d95d4177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_writer_level_profile(test_df, feature_store):\n",
    "\n",
    "  writer_names_in_fold = test_df['name'].unique()\n",
    "  print(writer_names_in_fold)\n",
    "  test_features_df = feature_store[feature_store['name'].isin(writer_names_in_fold)]\n",
    "  \n",
    "  writer_level_feature_df = test_features_df.groupby(['name'], as_index=False)[test_features_df.columns[:-2]].agg(['mean', 'median', 'std', 'skew']).fillna(0)\n",
    "  writer_level_feature_df.columns = writer_level_feature_df.columns.map(lambda x: '|'.join(map(str, x)))\n",
    "  \n",
    "  final_test_df = pd.merge(writer_level_feature_df, test_df[['name', 'NativeLanguage']], how='inner', left_on='name|', right_on='name')\n",
    "  \n",
    "  X_test = final_test_df.drop(columns=['name','name|','NativeLanguage'])\n",
    "  y_test = final_test_df['NativeLanguage']\n",
    "  y_test = np.vectorize(convert_y)(y_test)\n",
    "  \n",
    "  return X_test, y_test\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253f7d27",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a815e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:19:54,930] A new study created in memory with name: no-name-db838380-1444-4320-977f-9e7c79536846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented training set created with 342 samples.\n",
      "\n",
      "Starting Optuna hyperparameter search for 100 trials...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.835582:   1%|          | 1/100 [00:05<09:27,  5.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:20:00,663] Trial 0 finished with value: 0.8355819233868015 and parameters: {'feature_selection__max_features': 852, 'classification__n_estimators': 312, 'classification__learning_rate': 0.0011458831312373397, 'classification__max_depth': 3, 'classification__subsample': 0.8729355511782803, 'classification__colsample_bytree': 0.9654733928816991, 'classification__lambda': 2.016518655846322, 'classification__alpha': 4.628107463630652, 'classification__min_child_weight': 8}. Best is trial 0 with value: 0.8355819233868015.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.849394:   2%|▏         | 2/100 [00:11<09:33,  5.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:20:06,602] Trial 1 finished with value: 0.8493935989398785 and parameters: {'feature_selection__max_features': 945, 'classification__n_estimators': 236, 'classification__learning_rate': 0.001166701653573398, 'classification__max_depth': 9, 'classification__subsample': 0.9849435629343285, 'classification__colsample_bytree': 0.6624165964672706, 'classification__lambda': 1.0042465856375498, 'classification__alpha': 0.16180479257103458, 'classification__min_child_weight': 6}. Best is trial 1 with value: 0.8493935989398785.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.849394:   3%|▎         | 3/100 [00:14<06:57,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:20:09,058] Trial 2 finished with value: 0.7865613797817188 and parameters: {'feature_selection__max_features': 16, 'classification__n_estimators': 650, 'classification__learning_rate': 0.0019863447066253742, 'classification__max_depth': 11, 'classification__subsample': 0.6863467660030075, 'classification__colsample_bytree': 0.7524342306715951, 'classification__lambda': 18.737371748813043, 'classification__alpha': 0.11036834861869346, 'classification__min_child_weight': 1}. Best is trial 1 with value: 0.8493935989398785.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.880406:   4%|▍         | 4/100 [00:17<06:17,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:20:12,421] Trial 3 finished with value: 0.8804061435640383 and parameters: {'feature_selection__max_features': 652, 'classification__n_estimators': 106, 'classification__learning_rate': 0.009006522492857764, 'classification__max_depth': 10, 'classification__subsample': 0.9534074854108616, 'classification__colsample_bytree': 0.8955394500812728, 'classification__lambda': 0.6966269568188569, 'classification__alpha': 8.488396142030425, 'classification__min_child_weight': 4}. Best is trial 3 with value: 0.8804061435640383.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.880406:   5%|▌         | 5/100 [00:26<09:03,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:20:21,309] Trial 4 finished with value: 0.8333659696989476 and parameters: {'feature_selection__max_features': 687, 'classification__n_estimators': 396, 'classification__learning_rate': 0.004437523713150149, 'classification__max_depth': 11, 'classification__subsample': 0.9156738052239445, 'classification__colsample_bytree': 0.9601386627589964, 'classification__lambda': 0.3983235404389969, 'classification__alpha': 0.2449641100974264, 'classification__min_child_weight': 9}. Best is trial 3 with value: 0.8804061435640383.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.880406:   6%|▌         | 6/100 [00:39<13:06,  8.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:20:34,816] Trial 5 finished with value: 0.8661367319230937 and parameters: {'feature_selection__max_features': 497, 'classification__n_estimators': 821, 'classification__learning_rate': 0.002010423012213232, 'classification__max_depth': 4, 'classification__subsample': 0.9468279605651455, 'classification__colsample_bytree': 0.6045116993794709, 'classification__lambda': 0.4033564198268641, 'classification__alpha': 1.0937877157751683, 'classification__min_child_weight': 4}. Best is trial 3 with value: 0.8804061435640383.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.880406:   7%|▋         | 7/100 [00:48<13:14,  8.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:20:43,724] Trial 6 finished with value: 0.8333659696989476 and parameters: {'feature_selection__max_features': 681, 'classification__n_estimators': 433, 'classification__learning_rate': 0.0037077506194832404, 'classification__max_depth': 12, 'classification__subsample': 0.9034808549569937, 'classification__colsample_bytree': 0.7415220662819872, 'classification__lambda': 12.04961450673184, 'classification__alpha': 0.27700794806763945, 'classification__min_child_weight': 8}. Best is trial 3 with value: 0.8804061435640383.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.880406:   8%|▊         | 8/100 [00:52<10:42,  6.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:20:47,371] Trial 7 finished with value: 0.8638650890792873 and parameters: {'feature_selection__max_features': 896, 'classification__n_estimators': 192, 'classification__learning_rate': 0.009112021818006343, 'classification__max_depth': 6, 'classification__subsample': 0.6240684887807985, 'classification__colsample_bytree': 0.844169278102954, 'classification__lambda': 1.960612905116954, 'classification__alpha': 1.3050476188412212, 'classification__min_child_weight': 10}. Best is trial 3 with value: 0.8804061435640383.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.880406:   9%|▉         | 9/100 [00:55<08:42,  5.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:20:50,385] Trial 8 finished with value: 0.8481906124247743 and parameters: {'feature_selection__max_features': 143, 'classification__n_estimators': 377, 'classification__learning_rate': 0.004974576536035103, 'classification__max_depth': 10, 'classification__subsample': 0.8167168986711639, 'classification__colsample_bytree': 0.8475417019904288, 'classification__lambda': 10.576519490280266, 'classification__alpha': 6.100531284373446, 'classification__min_child_weight': 5}. Best is trial 3 with value: 0.8804061435640383.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 9. Best value: 0.896686:  10%|█         | 10/100 [01:08<11:52,  7.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:21:03,176] Trial 9 finished with value: 0.8966861598440546 and parameters: {'feature_selection__max_features': 540, 'classification__n_estimators': 923, 'classification__learning_rate': 0.0025379468840938985, 'classification__max_depth': 8, 'classification__subsample': 0.6656903571409114, 'classification__colsample_bytree': 0.7425450129315556, 'classification__lambda': 3.1486062101263026, 'classification__alpha': 0.420599598504815, 'classification__min_child_weight': 10}. Best is trial 9 with value: 0.8966861598440546.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 9. Best value: 0.896686:  11%|█         | 11/100 [01:31<18:43, 12.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:21:26,457] Trial 10 finished with value: 0.8817125064117848 and parameters: {'feature_selection__max_features': 357, 'classification__n_estimators': 990, 'classification__learning_rate': 0.0021469332913163853, 'classification__max_depth': 14, 'classification__subsample': 0.719897199835535, 'classification__colsample_bytree': 0.6968269236158928, 'classification__lambda': 0.11949555226044213, 'classification__alpha': 0.8086918277451611, 'classification__min_child_weight': 1}. Best is trial 9 with value: 0.8966861598440546.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 9. Best value: 0.896686:  12%|█▏        | 12/100 [01:53<22:52, 15.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:21:48,842] Trial 11 finished with value: 0.8817125064117848 and parameters: {'feature_selection__max_features': 333, 'classification__n_estimators': 972, 'classification__learning_rate': 0.002189281116714064, 'classification__max_depth': 15, 'classification__subsample': 0.7201613568083168, 'classification__colsample_bytree': 0.6959938966269206, 'classification__lambda': 0.10052956630871676, 'classification__alpha': 0.5970182632740371, 'classification__min_child_weight': 1}. Best is trial 9 with value: 0.8966861598440546.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 9. Best value: 0.896686:  13%|█▎        | 13/100 [02:10<23:15, 16.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:22:05,908] Trial 12 finished with value: 0.8817125064117848 and parameters: {'feature_selection__max_features': 342, 'classification__n_estimators': 977, 'classification__learning_rate': 0.00269211736231555, 'classification__max_depth': 7, 'classification__subsample': 0.7154356961068431, 'classification__colsample_bytree': 0.7721092135646448, 'classification__lambda': 4.748216088750303, 'classification__alpha': 0.5472269497585609, 'classification__min_child_weight': 3}. Best is trial 9 with value: 0.8966861598440546.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 13. Best value: 0.896913:  14%|█▍        | 14/100 [02:18<19:27, 13.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:22:13,783] Trial 13 finished with value: 0.8969128246974023 and parameters: {'feature_selection__max_features': 389, 'classification__n_estimators': 707, 'classification__learning_rate': 0.002964299538561056, 'classification__max_depth': 15, 'classification__subsample': 0.6008311504826784, 'classification__colsample_bytree': 0.687582002074241, 'classification__lambda': 0.11133559321582051, 'classification__alpha': 2.6272515412603163, 'classification__min_child_weight': 7}. Best is trial 13 with value: 0.8969128246974023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 13. Best value: 0.896913:  15%|█▌        | 15/100 [02:25<16:21, 11.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:22:20,646] Trial 14 finished with value: 0.8318497392907557 and parameters: {'feature_selection__max_features': 512, 'classification__n_estimators': 711, 'classification__learning_rate': 0.00570500074320415, 'classification__max_depth': 7, 'classification__subsample': 0.6025863435809103, 'classification__colsample_bytree': 0.622930628436617, 'classification__lambda': 3.2418061019572266, 'classification__alpha': 18.745594270639437, 'classification__min_child_weight': 7}. Best is trial 13 with value: 0.8969128246974023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 13. Best value: 0.896913:  16%|█▌        | 16/100 [02:35<15:24, 11.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:22:30,401] Trial 15 finished with value: 0.8969128246974023 and parameters: {'feature_selection__max_features': 490, 'classification__n_estimators': 795, 'classification__learning_rate': 0.0030152909739547045, 'classification__max_depth': 8, 'classification__subsample': 0.6528036618637006, 'classification__colsample_bytree': 0.8197352838451871, 'classification__lambda': 4.50608369642987, 'classification__alpha': 2.6061554579293222, 'classification__min_child_weight': 10}. Best is trial 13 with value: 0.8969128246974023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 13. Best value: 0.896913:  17%|█▋        | 17/100 [02:40<12:35,  9.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:22:35,066] Trial 16 finished with value: 0.8481906124247743 and parameters: {'feature_selection__max_features': 219, 'classification__n_estimators': 590, 'classification__learning_rate': 0.003407894176000691, 'classification__max_depth': 13, 'classification__subsample': 0.7798050357023875, 'classification__colsample_bytree': 0.8183218678994293, 'classification__lambda': 6.592288422478148, 'classification__alpha': 2.771266918606883, 'classification__min_child_weight': 7}. Best is trial 13 with value: 0.8969128246974023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 13. Best value: 0.896913:  18%|█▊        | 18/100 [02:50<13:00,  9.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:22:45,554] Trial 17 finished with value: 0.8804061435640383 and parameters: {'feature_selection__max_features': 454, 'classification__n_estimators': 804, 'classification__learning_rate': 0.0014513752273978306, 'classification__max_depth': 5, 'classification__subsample': 0.646680558490103, 'classification__colsample_bytree': 0.9078076891584782, 'classification__lambda': 0.21037605322410094, 'classification__alpha': 2.4176732854640757, 'classification__min_child_weight': 9}. Best is trial 13 with value: 0.8969128246974023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 13. Best value: 0.896913:  19%|█▉        | 19/100 [02:56<11:19,  8.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:22:51,296] Trial 18 finished with value: 0.8802606434185382 and parameters: {'feature_selection__max_features': 245, 'classification__n_estimators': 515, 'classification__learning_rate': 0.006897316662326304, 'classification__max_depth': 15, 'classification__subsample': 0.7664904311236891, 'classification__colsample_bytree': 0.7974160445854115, 'classification__lambda': 0.9082655413794732, 'classification__alpha': 2.203279086981557, 'classification__min_child_weight': 6}. Best is trial 13 with value: 0.8969128246974023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 13. Best value: 0.896913:  20%|██        | 20/100 [03:08<12:43,  9.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:23:03,550] Trial 19 finished with value: 0.8638650890792873 and parameters: {'feature_selection__max_features': 796, 'classification__n_estimators': 769, 'classification__learning_rate': 0.0014879091638177926, 'classification__max_depth': 13, 'classification__subsample': 0.607230421996074, 'classification__colsample_bytree': 0.6992316204959881, 'classification__lambda': 29.83211565018962, 'classification__alpha': 13.219013746475204, 'classification__min_child_weight': 8}. Best is trial 13 with value: 0.8969128246974023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 13. Best value: 0.896913:  21%|██        | 21/100 [03:22<14:23, 10.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:23:17,688] Trial 20 finished with value: 0.8802606434185382 and parameters: {'feature_selection__max_features': 607, 'classification__n_estimators': 878, 'classification__learning_rate': 0.00405901372365251, 'classification__max_depth': 8, 'classification__subsample': 0.8328694706218455, 'classification__colsample_bytree': 0.8884172560865086, 'classification__lambda': 0.2869809868829398, 'classification__alpha': 3.9739627200095895, 'classification__min_child_weight': 10}. Best is trial 13 with value: 0.8969128246974023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 13. Best value: 0.896913:  22%|██▏       | 22/100 [03:31<13:27, 10.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:23:26,723] Trial 21 finished with value: 0.8969128246974023 and parameters: {'feature_selection__max_features': 435, 'classification__n_estimators': 880, 'classification__learning_rate': 0.0026756334025371635, 'classification__max_depth': 8, 'classification__subsample': 0.6626499164543843, 'classification__colsample_bytree': 0.7284945415718735, 'classification__lambda': 3.5534793529123325, 'classification__alpha': 1.651436397281431, 'classification__min_child_weight': 10}. Best is trial 13 with value: 0.8969128246974023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 13. Best value: 0.896913:  23%|██▎       | 23/100 [03:38<11:47,  9.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:23:33,190] Trial 22 finished with value: 0.8802606434185382 and parameters: {'feature_selection__max_features': 417, 'classification__n_estimators': 677, 'classification__learning_rate': 0.0031849486371116464, 'classification__max_depth': 9, 'classification__subsample': 0.6519969237602138, 'classification__colsample_bytree': 0.6530563767746224, 'classification__lambda': 6.294501222091664, 'classification__alpha': 1.711078500921907, 'classification__min_child_weight': 9}. Best is trial 13 with value: 0.8969128246974023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 13. Best value: 0.896913:  24%|██▍       | 24/100 [03:50<12:51, 10.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:23:45,575] Trial 23 finished with value: 0.8638650890792873 and parameters: {'feature_selection__max_features': 588, 'classification__n_estimators': 865, 'classification__learning_rate': 0.0026633875433455176, 'classification__max_depth': 6, 'classification__subsample': 0.6910008615014398, 'classification__colsample_bytree': 0.7816973860252203, 'classification__lambda': 1.4090752734321612, 'classification__alpha': 3.5764327046528814, 'classification__min_child_weight': 10}. Best is trial 13 with value: 0.8969128246974023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 13. Best value: 0.896913:  25%|██▌       | 25/100 [03:57<11:37,  9.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:23:52,910] Trial 24 finished with value: 0.8804061435640383 and parameters: {'feature_selection__max_features': 257, 'classification__n_estimators': 741, 'classification__learning_rate': 0.0028511871124928386, 'classification__max_depth': 8, 'classification__subsample': 0.7493013587273256, 'classification__colsample_bytree': 0.7088817229145956, 'classification__lambda': 3.791276861860396, 'classification__alpha': 1.6195581326654962, 'classification__min_child_weight': 7}. Best is trial 13 with value: 0.8969128246974023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 13. Best value: 0.896913:  26%|██▌       | 26/100 [04:04<10:20,  8.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:23:59,129] Trial 25 finished with value: 0.8804061435640383 and parameters: {'feature_selection__max_features': 423, 'classification__n_estimators': 569, 'classification__learning_rate': 0.0015289004242457268, 'classification__max_depth': 10, 'classification__subsample': 0.642842679883533, 'classification__colsample_bytree': 0.6632133003186635, 'classification__lambda': 10.328892845834032, 'classification__alpha': 7.2912325384046515, 'classification__min_child_weight': 9}. Best is trial 13 with value: 0.8969128246974023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 13. Best value: 0.896913:  27%|██▋       | 27/100 [04:17<11:51,  9.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:24:12,053] Trial 26 finished with value: 0.8638650890792873 and parameters: {'feature_selection__max_features': 767, 'classification__n_estimators': 645, 'classification__learning_rate': 0.003275010511983655, 'classification__max_depth': 6, 'classification__subsample': 0.6767226150860873, 'classification__colsample_bytree': 0.8262601499741494, 'classification__lambda': 2.4360160847997867, 'classification__alpha': 0.9647543318235792, 'classification__min_child_weight': 8}. Best is trial 13 with value: 0.8969128246974023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 13. Best value: 0.896913:  28%|██▊       | 28/100 [04:21<09:43,  8.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:24:16,354] Trial 27 finished with value: 0.8781043517885623 and parameters: {'feature_selection__max_features': 126, 'classification__n_estimators': 872, 'classification__learning_rate': 0.0051949467477691875, 'classification__max_depth': 7, 'classification__subsample': 0.6280709675209359, 'classification__colsample_bytree': 0.748712064154569, 'classification__lambda': 1.336097126466459, 'classification__alpha': 1.9233375788654772, 'classification__min_child_weight': 9}. Best is trial 13 with value: 0.8969128246974023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 28. Best value: 0.91179:  29%|██▉       | 29/100 [04:33<11:03,  9.34s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:24:28,563] Trial 28 finished with value: 0.91178976062697 and parameters: {'feature_selection__max_features': 395, 'classification__n_estimators': 775, 'classification__learning_rate': 0.00401922894304209, 'classification__max_depth': 11, 'classification__subsample': 0.6952662913482198, 'classification__colsample_bytree': 0.7193605514760858, 'classification__lambda': 6.351663967363323, 'classification__alpha': 3.5358651161326615, 'classification__min_child_weight': 5}. Best is trial 28 with value: 0.91178976062697.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 28. Best value: 0.91179:  30%|███       | 30/100 [04:43<11:00,  9.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:24:38,230] Trial 29 finished with value: 0.8481906124247743 and parameters: {'feature_selection__max_features': 315, 'classification__n_estimators': 765, 'classification__learning_rate': 0.004137838161052336, 'classification__max_depth': 13, 'classification__subsample': 0.7412386067565334, 'classification__colsample_bytree': 0.9928758978757156, 'classification__lambda': 6.067823838759666, 'classification__alpha': 4.694552149519725, 'classification__min_child_weight': 5}. Best is trial 28 with value: 0.91178976062697.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 28. Best value: 0.91179:  31%|███       | 31/100 [04:49<09:46,  8.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:24:44,534] Trial 30 finished with value: 0.8474391105970053 and parameters: {'feature_selection__max_features': 185, 'classification__n_estimators': 501, 'classification__learning_rate': 0.006472586624572672, 'classification__max_depth': 12, 'classification__subsample': 0.8478807227013911, 'classification__colsample_bytree': 0.8707698172466091, 'classification__lambda': 16.118543308329894, 'classification__alpha': 11.28228612951167, 'classification__min_child_weight': 3}. Best is trial 28 with value: 0.91178976062697.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 28. Best value: 0.91179:  32%|███▏      | 32/100 [04:58<09:45,  8.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:24:53,404] Trial 31 finished with value: 0.8804061435640383 and parameters: {'feature_selection__max_features': 425, 'classification__n_estimators': 818, 'classification__learning_rate': 0.0024031354611009047, 'classification__max_depth': 3, 'classification__subsample': 0.7001633931309227, 'classification__colsample_bytree': 0.7248036776916988, 'classification__lambda': 2.525009574520679, 'classification__alpha': 3.629534201072595, 'classification__min_child_weight': 6}. Best is trial 28 with value: 0.91178976062697.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 28. Best value: 0.91179:  33%|███▎      | 33/100 [05:15<12:22, 11.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:25:10,276] Trial 32 finished with value: 0.8792935792935793 and parameters: {'feature_selection__max_features': 565, 'classification__n_estimators': 912, 'classification__learning_rate': 0.002973108707530996, 'classification__max_depth': 9, 'classification__subsample': 0.6011226109399452, 'classification__colsample_bytree': 0.6630031185254265, 'classification__lambda': 4.520157193609762, 'classification__alpha': 5.326336137119816, 'classification__min_child_weight': 5}. Best is trial 28 with value: 0.91178976062697.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 28. Best value: 0.91179:  34%|███▍      | 34/100 [05:28<12:55, 11.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:25:23,557] Trial 33 finished with value: 0.8652666148128944 and parameters: {'feature_selection__max_features': 464, 'classification__n_estimators': 685, 'classification__learning_rate': 0.001802066634571583, 'classification__max_depth': 9, 'classification__subsample': 0.6650200178450877, 'classification__colsample_bytree': 0.7805175973915408, 'classification__lambda': 7.857623775684581, 'classification__alpha': 2.7852133765304545, 'classification__min_child_weight': 4}. Best is trial 28 with value: 0.91178976062697.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 28. Best value: 0.91179:  35%|███▌      | 35/100 [05:36<11:20, 10.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:25:31,016] Trial 34 finished with value: 0.8802606434185382 and parameters: {'feature_selection__max_features': 386, 'classification__n_estimators': 622, 'classification__learning_rate': 0.003697692977104419, 'classification__max_depth': 11, 'classification__subsample': 0.6304447571538656, 'classification__colsample_bytree': 0.6391575718878683, 'classification__lambda': 0.7308029726630045, 'classification__alpha': 1.3278809690499471, 'classification__min_child_weight': 8}. Best is trial 28 with value: 0.91178976062697.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 28. Best value: 0.91179:  36%|███▌      | 36/100 [05:46<11:04, 10.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:25:41,235] Trial 35 finished with value: 0.847091110249005 and parameters: {'feature_selection__max_features': 274, 'classification__n_estimators': 732, 'classification__learning_rate': 0.004654514162136569, 'classification__max_depth': 14, 'classification__subsample': 0.6905919659868428, 'classification__colsample_bytree': 0.6791360523407458, 'classification__lambda': 4.810028569688741, 'classification__alpha': 3.3697638508501906, 'classification__min_child_weight': 6}. Best is trial 28 with value: 0.91178976062697.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 28. Best value: 0.91179:  37%|███▋      | 37/100 [06:04<13:23, 12.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:25:59,539] Trial 36 finished with value: 0.8952380952380953 and parameters: {'feature_selection__max_features': 490, 'classification__n_estimators': 805, 'classification__learning_rate': 0.00185820921204577, 'classification__max_depth': 8, 'classification__subsample': 0.668882841892033, 'classification__colsample_bytree': 0.7227194135929418, 'classification__lambda': 1.5365743604607902, 'classification__alpha': 8.060906878386572, 'classification__min_child_weight': 3}. Best is trial 28 with value: 0.91178976062697.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 28. Best value: 0.91179:  38%|███▊      | 38/100 [06:25<15:33, 15.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:26:19,949] Trial 37 finished with value: 0.8802606434185382 and parameters: {'feature_selection__max_features': 996, 'classification__n_estimators': 925, 'classification__learning_rate': 0.0035993187580767093, 'classification__max_depth': 11, 'classification__subsample': 0.630106490450429, 'classification__colsample_bytree': 0.7666550819378738, 'classification__lambda': 0.531286996401663, 'classification__alpha': 2.0058081725024164, 'classification__min_child_weight': 7}. Best is trial 28 with value: 0.91178976062697.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 28. Best value: 0.91179:  39%|███▉      | 39/100 [06:28<11:49, 11.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:26:23,589] Trial 38 finished with value: 0.8481906124247743 and parameters: {'feature_selection__max_features': 56, 'classification__n_estimators': 850, 'classification__learning_rate': 0.001154849611157281, 'classification__max_depth': 10, 'classification__subsample': 0.7381354383441577, 'classification__colsample_bytree': 0.8113687194129668, 'classification__lambda': 14.567009581424077, 'classification__alpha': 0.7722516746704499, 'classification__min_child_weight': 10}. Best is trial 28 with value: 0.91178976062697.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 28. Best value: 0.91179:  40%|████      | 40/100 [06:46<13:22, 13.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:26:41,049] Trial 39 finished with value: 0.8638650890792873 and parameters: {'feature_selection__max_features': 649, 'classification__n_estimators': 577, 'classification__learning_rate': 0.002351389207759003, 'classification__max_depth': 12, 'classification__subsample': 0.6534804375921152, 'classification__colsample_bytree': 0.9190567952642499, 'classification__lambda': 21.132276344628043, 'classification__alpha': 0.10322862967571123, 'classification__min_child_weight': 4}. Best is trial 28 with value: 0.91178976062697.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 28. Best value: 0.91179:  41%|████      | 41/100 [07:00<13:25, 13.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:26:55,318] Trial 40 finished with value: 0.8638650890792873 and parameters: {'feature_selection__max_features': 291, 'classification__n_estimators': 774, 'classification__learning_rate': 0.004135494419052138, 'classification__max_depth': 10, 'classification__subsample': 0.7103044761969719, 'classification__colsample_bytree': 0.7301938326700387, 'classification__lambda': 8.699689794947359, 'classification__alpha': 1.1864378118807863, 'classification__min_child_weight': 2}. Best is trial 28 with value: 0.91178976062697.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 28. Best value: 0.91179:  42%|████▏     | 42/100 [07:14<13:21, 13.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:27:09,531] Trial 41 finished with value: 0.8969128246974023 and parameters: {'feature_selection__max_features': 510, 'classification__n_estimators': 931, 'classification__learning_rate': 0.0024700773937021783, 'classification__max_depth': 8, 'classification__subsample': 0.6760906040392138, 'classification__colsample_bytree': 0.7476528620839865, 'classification__lambda': 3.1611096284475524, 'classification__alpha': 0.19004853081133843, 'classification__min_child_weight': 10}. Best is trial 28 with value: 0.91178976062697.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 28. Best value: 0.91179:  43%|████▎     | 43/100 [07:28<13:09, 13.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:27:23,464] Trial 42 finished with value: 0.8966861598440546 and parameters: {'feature_selection__max_features': 522, 'classification__n_estimators': 933, 'classification__learning_rate': 0.00286619818731405, 'classification__max_depth': 7, 'classification__subsample': 0.678696648080516, 'classification__colsample_bytree': 0.7585286554097248, 'classification__lambda': 2.564545596591122, 'classification__alpha': 0.17600011002139482, 'classification__min_child_weight': 10}. Best is trial 28 with value: 0.91178976062697.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  44%|████▍     | 44/100 [07:38<11:51, 12.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:27:33,507] Trial 43 finished with value: 0.9134301765880712 and parameters: {'feature_selection__max_features': 396, 'classification__n_estimators': 826, 'classification__learning_rate': 0.003050520176067708, 'classification__max_depth': 5, 'classification__subsample': 0.6186272156342054, 'classification__colsample_bytree': 0.7934754900372663, 'classification__lambda': 3.262422217867939, 'classification__alpha': 0.3223021165552474, 'classification__min_child_weight': 9}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  45%|████▌     | 45/100 [07:42<09:18, 10.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:27:37,720] Trial 44 finished with value: 0.8638650890792873 and parameters: {'feature_selection__max_features': 371, 'classification__n_estimators': 262, 'classification__learning_rate': 0.003337034729350917, 'classification__max_depth': 4, 'classification__subsample': 0.6194454649284004, 'classification__colsample_bytree': 0.8474038507187974, 'classification__lambda': 2.1008034728516662, 'classification__alpha': 0.3192860085494674, 'classification__min_child_weight': 9}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  46%|████▌     | 46/100 [07:52<09:03, 10.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:27:47,555] Trial 45 finished with value: 0.8638650890792873 and parameters: {'feature_selection__max_features': 391, 'classification__n_estimators': 830, 'classification__learning_rate': 0.0030665267111436843, 'classification__max_depth': 5, 'classification__subsample': 0.6163933578519958, 'classification__colsample_bytree': 0.7963041952554138, 'classification__lambda': 1.0122923677805016, 'classification__alpha': 5.432168757029556, 'classification__min_child_weight': 9}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  47%|████▋     | 47/100 [08:01<08:35,  9.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:27:56,459] Trial 46 finished with value: 0.8802606434185382 and parameters: {'feature_selection__max_features': 473, 'classification__n_estimators': 695, 'classification__learning_rate': 0.003783635463778459, 'classification__max_depth': 5, 'classification__subsample': 0.6397542089844437, 'classification__colsample_bytree': 0.679019941582859, 'classification__lambda': 4.708367763104875, 'classification__alpha': 2.708984883161488, 'classification__min_child_weight': 8}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  48%|████▊     | 48/100 [08:10<08:18,  9.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:28:05,730] Trial 47 finished with value: 0.8158788328279853 and parameters: {'feature_selection__max_features': 312, 'classification__n_estimators': 890, 'classification__learning_rate': 0.002169434731217347, 'classification__max_depth': 3, 'classification__subsample': 0.9897272210557941, 'classification__colsample_bytree': 0.8289043927820645, 'classification__lambda': 3.7789212396954945, 'classification__alpha': 0.9365466194731309, 'classification__min_child_weight': 9}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  49%|████▉     | 49/100 [08:21<08:33, 10.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:28:16,911] Trial 48 finished with value: 0.8817125064117848 and parameters: {'feature_selection__max_features': 353, 'classification__n_estimators': 725, 'classification__learning_rate': 0.00267619277745132, 'classification__max_depth': 14, 'classification__subsample': 0.6570751893042887, 'classification__colsample_bytree': 0.8660257948520693, 'classification__lambda': 0.15190599454808215, 'classification__alpha': 0.5826607287054075, 'classification__min_child_weight': 5}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  50%|█████     | 50/100 [08:24<06:30,  7.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:28:19,503] Trial 49 finished with value: 0.8487776124445595 and parameters: {'feature_selection__max_features': 628, 'classification__n_estimators': 73, 'classification__learning_rate': 0.004453893037138067, 'classification__max_depth': 11, 'classification__subsample': 0.8994495816926549, 'classification__colsample_bytree': 0.7178101145130811, 'classification__lambda': 7.883221316403771, 'classification__alpha': 0.4087953970305683, 'classification__min_child_weight': 6}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  51%|█████     | 51/100 [08:33<06:41,  8.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:28:28,592] Trial 50 finished with value: 0.8633528265107212 and parameters: {'feature_selection__max_features': 721, 'classification__n_estimators': 459, 'classification__learning_rate': 0.005416358683460615, 'classification__max_depth': 15, 'classification__subsample': 0.7845474411242199, 'classification__colsample_bytree': 0.618427278261625, 'classification__lambda': 1.8590645061985946, 'classification__alpha': 3.0303240015370307, 'classification__min_child_weight': 8}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  52%|█████▏    | 52/100 [08:46<07:34,  9.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:28:40,993] Trial 51 finished with value: 0.8633528265107212 and parameters: {'feature_selection__max_features': 557, 'classification__n_estimators': 950, 'classification__learning_rate': 0.0024495345739278815, 'classification__max_depth': 9, 'classification__subsample': 0.636923464172569, 'classification__colsample_bytree': 0.6832809653742722, 'classification__lambda': 3.2429174140492143, 'classification__alpha': 0.14915212328168057, 'classification__min_child_weight': 10}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  53%|█████▎    | 53/100 [08:58<08:09, 10.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:28:53,634] Trial 52 finished with value: 0.8969128246974023 and parameters: {'feature_selection__max_features': 453, 'classification__n_estimators': 991, 'classification__learning_rate': 0.002048150155324929, 'classification__max_depth': 8, 'classification__subsample': 0.6975744456043096, 'classification__colsample_bytree': 0.738330030420407, 'classification__lambda': 5.593122172217202, 'classification__alpha': 0.19711940196391148, 'classification__min_child_weight': 10}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  54%|█████▍    | 54/100 [09:11<08:29, 11.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:29:06,257] Trial 53 finished with value: 0.8802606434185382 and parameters: {'feature_selection__max_features': 538, 'classification__n_estimators': 783, 'classification__learning_rate': 0.002294188972387718, 'classification__max_depth': 7, 'classification__subsample': 0.6162190868813048, 'classification__colsample_bytree': 0.7947001987572306, 'classification__lambda': 3.0552653921760706, 'classification__alpha': 0.1326245874292284, 'classification__min_child_weight': 10}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  55%|█████▌    | 55/100 [09:24<08:46, 11.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:29:19,421] Trial 54 finished with value: 0.8969128246974023 and parameters: {'feature_selection__max_features': 498, 'classification__n_estimators': 901, 'classification__learning_rate': 0.0017059520067751967, 'classification__max_depth': 6, 'classification__subsample': 0.6748526363733272, 'classification__colsample_bytree': 0.7649385601878154, 'classification__lambda': 3.4739972713220837, 'classification__alpha': 0.2398949063432325, 'classification__min_child_weight': 10}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  56%|█████▌    | 56/100 [09:34<08:14, 11.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:29:29,594] Trial 55 finished with value: 0.8969128246974023 and parameters: {'feature_selection__max_features': 415, 'classification__n_estimators': 843, 'classification__learning_rate': 0.002618042064740913, 'classification__max_depth': 8, 'classification__subsample': 0.7221208315718235, 'classification__colsample_bytree': 0.7052686311444942, 'classification__lambda': 4.123348150928312, 'classification__alpha': 1.5596814350980186, 'classification__min_child_weight': 9}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  57%|█████▋    | 57/100 [09:44<07:42, 10.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:29:39,203] Trial 56 finished with value: 0.898177174039243 and parameters: {'feature_selection__max_features': 441, 'classification__n_estimators': 792, 'classification__learning_rate': 0.0034804870295616457, 'classification__max_depth': 4, 'classification__subsample': 0.6585125001205603, 'classification__colsample_bytree': 0.7444803412435206, 'classification__lambda': 1.152313168569403, 'classification__alpha': 0.33376148165362834, 'classification__min_child_weight': 9}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  58%|█████▊    | 58/100 [09:56<07:49, 11.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:29:51,349] Trial 57 finished with value: 0.8497698667190193 and parameters: {'feature_selection__max_features': 435, 'classification__n_estimators': 650, 'classification__learning_rate': 0.0034989662245481796, 'classification__max_depth': 4, 'classification__subsample': 0.9593304581469422, 'classification__colsample_bytree': 0.809040179595727, 'classification__lambda': 0.32696701197362915, 'classification__alpha': 0.4347007052016737, 'classification__min_child_weight': 7}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  59%|█████▉    | 59/100 [10:05<07:09, 10.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:30:00,192] Trial 58 finished with value: 0.8802606434185382 and parameters: {'feature_selection__max_features': 350, 'classification__n_estimators': 741, 'classification__learning_rate': 0.003933013576391759, 'classification__max_depth': 4, 'classification__subsample': 0.6511810847611833, 'classification__colsample_bytree': 0.7822630701447371, 'classification__lambda': 1.1530913055724041, 'classification__alpha': 4.218280427123801, 'classification__min_child_weight': 8}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  60%|██████    | 60/100 [10:14<06:47, 10.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:30:09,710] Trial 59 finished with value: 0.8802606434185382 and parameters: {'feature_selection__max_features': 400, 'classification__n_estimators': 793, 'classification__learning_rate': 0.0031282375596168505, 'classification__max_depth': 5, 'classification__subsample': 0.6050971605732489, 'classification__colsample_bytree': 0.7370843613846245, 'classification__lambda': 10.900005432201631, 'classification__alpha': 0.7244070605492977, 'classification__min_child_weight': 9}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  61%|██████    | 61/100 [10:26<06:51, 10.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:30:21,077] Trial 60 finished with value: 0.8802606434185382 and parameters: {'feature_selection__max_features': 596, 'classification__n_estimators': 607, 'classification__learning_rate': 0.004781449704436727, 'classification__max_depth': 6, 'classification__subsample': 0.6615975701418042, 'classification__colsample_bytree': 0.6838646858490137, 'classification__lambda': 0.5029133210525534, 'classification__alpha': 2.4047310234985635, 'classification__min_child_weight': 7}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  62%|██████▏   | 62/100 [10:38<07:06, 11.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:30:33,856] Trial 61 finished with value: 0.9134301765880712 and parameters: {'feature_selection__max_features': 497, 'classification__n_estimators': 959, 'classification__learning_rate': 0.0028579934157133423, 'classification__max_depth': 7, 'classification__subsample': 0.6847078679483505, 'classification__colsample_bytree': 0.7489325438000831, 'classification__lambda': 2.701312646285296, 'classification__alpha': 0.3237820287487846, 'classification__min_child_weight': 10}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  63%|██████▎   | 63/100 [10:51<07:08, 11.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:30:46,259] Trial 62 finished with value: 0.9134301765880712 and parameters: {'feature_selection__max_features': 459, 'classification__n_estimators': 968, 'classification__learning_rate': 0.00285652358387951, 'classification__max_depth': 7, 'classification__subsample': 0.7044343596989254, 'classification__colsample_bytree': 0.7148290476918823, 'classification__lambda': 0.211194422467877, 'classification__alpha': 0.2870288604931385, 'classification__min_child_weight': 9}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  64%|██████▍   | 64/100 [11:06<07:36, 12.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:31:01,513] Trial 63 finished with value: 0.9134301765880712 and parameters: {'feature_selection__max_features': 493, 'classification__n_estimators': 956, 'classification__learning_rate': 0.0029044954838025328, 'classification__max_depth': 7, 'classification__subsample': 0.710668957489906, 'classification__colsample_bytree': 0.7166123376722005, 'classification__lambda': 0.16181715923904547, 'classification__alpha': 0.30597200341781355, 'classification__min_child_weight': 9}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  65%|██████▌   | 65/100 [11:20<07:40, 13.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:31:15,803] Trial 64 finished with value: 0.8969128246974023 and parameters: {'feature_selection__max_features': 460, 'classification__n_estimators': 996, 'classification__learning_rate': 0.002936519814209154, 'classification__max_depth': 6, 'classification__subsample': 0.7687865700183834, 'classification__colsample_bytree': 0.7108865363267245, 'classification__lambda': 0.191561313226797, 'classification__alpha': 0.3146944844657309, 'classification__min_child_weight': 9}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  66%|██████▌   | 66/100 [11:32<07:16, 12.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:31:27,866] Trial 65 finished with value: 0.8804061435640383 and parameters: {'feature_selection__max_features': 323, 'classification__n_estimators': 952, 'classification__learning_rate': 0.003397621062900879, 'classification__max_depth': 5, 'classification__subsample': 0.7268205099029932, 'classification__colsample_bytree': 0.753261741555923, 'classification__lambda': 0.11197839557837075, 'classification__alpha': 0.23067068772080235, 'classification__min_child_weight': 8}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  67%|██████▋   | 67/100 [11:43<06:38, 12.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:31:38,197] Trial 66 finished with value: 0.8969128246974023 and parameters: {'feature_selection__max_features': 381, 'classification__n_estimators': 966, 'classification__learning_rate': 0.0028146164339356537, 'classification__max_depth': 3, 'classification__subsample': 0.7111163980041378, 'classification__colsample_bytree': 0.698670731338057, 'classification__lambda': 0.1497813840374051, 'classification__alpha': 0.37063658622375606, 'classification__min_child_weight': 9}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  68%|██████▊   | 68/100 [11:50<05:41, 10.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:31:45,540] Trial 67 finished with value: 0.8804061435640383 and parameters: {'feature_selection__max_features': 222, 'classification__n_estimators': 853, 'classification__learning_rate': 0.004275806785621101, 'classification__max_depth': 7, 'classification__subsample': 0.7554367782409555, 'classification__colsample_bytree': 0.6491331341835602, 'classification__lambda': 0.21304612213942042, 'classification__alpha': 0.5103495016879311, 'classification__min_child_weight': 9}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  69%|██████▉   | 69/100 [12:08<06:40, 12.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:32:03,716] Trial 68 finished with value: 0.8661367319230937 and parameters: {'feature_selection__max_features': 480, 'classification__n_estimators': 897, 'classification__learning_rate': 0.003255133045766197, 'classification__max_depth': 5, 'classification__subsample': 0.6897852216958031, 'classification__colsample_bytree': 0.6947595918559936, 'classification__lambda': 0.27868472442201986, 'classification__alpha': 0.2745309834032209, 'classification__min_child_weight': 5}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  70%|███████   | 70/100 [12:23<06:44, 13.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:32:18,523] Trial 69 finished with value: 0.8802606434185382 and parameters: {'feature_selection__max_features': 579, 'classification__n_estimators': 827, 'classification__learning_rate': 0.003729912121711444, 'classification__max_depth': 4, 'classification__subsample': 0.6976972171669596, 'classification__colsample_bytree': 0.7142333653880477, 'classification__lambda': 0.1302267542028042, 'classification__alpha': 0.4942919211510524, 'classification__min_child_weight': 8}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  71%|███████   | 71/100 [12:45<07:42, 15.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:32:40,181] Trial 70 finished with value: 0.8817125064117848 and parameters: {'feature_selection__max_features': 537, 'classification__n_estimators': 757, 'classification__learning_rate': 0.0031151766861265496, 'classification__max_depth': 6, 'classification__subsample': 0.736051199149735, 'classification__colsample_bytree': 0.7783187601543917, 'classification__lambda': 0.1789478621808354, 'classification__alpha': 0.31099765083904646, 'classification__min_child_weight': 4}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  72%|███████▏  | 72/100 [12:57<06:58, 14.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:32:52,861] Trial 71 finished with value: 0.9134301765880712 and parameters: {'feature_selection__max_features': 438, 'classification__n_estimators': 866, 'classification__learning_rate': 0.0028206764214767166, 'classification__max_depth': 7, 'classification__subsample': 0.6402318930337925, 'classification__colsample_bytree': 0.8339231642949809, 'classification__lambda': 0.2389216926719633, 'classification__alpha': 0.21948227100325157, 'classification__min_child_weight': 10}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  73%|███████▎  | 73/100 [13:11<06:28, 14.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:33:05,935] Trial 72 finished with value: 0.9134301765880712 and parameters: {'feature_selection__max_features': 434, 'classification__n_estimators': 866, 'classification__learning_rate': 0.002787419006501043, 'classification__max_depth': 7, 'classification__subsample': 0.6433997262925532, 'classification__colsample_bytree': 0.83919746219425, 'classification__lambda': 0.26445913947949307, 'classification__alpha': 0.12699683379736057, 'classification__min_child_weight': 10}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  74%|███████▍  | 74/100 [13:27<06:32, 15.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:33:22,651] Trial 73 finished with value: 0.9134301765880712 and parameters: {'feature_selection__max_features': 448, 'classification__n_estimators': 963, 'classification__learning_rate': 0.0028079993559580667, 'classification__max_depth': 7, 'classification__subsample': 0.7075952203469311, 'classification__colsample_bytree': 0.8618432817112214, 'classification__lambda': 0.24398201998678593, 'classification__alpha': 0.13273540562778469, 'classification__min_child_weight': 10}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  75%|███████▌  | 75/100 [13:36<05:30, 13.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:33:31,548] Trial 74 finished with value: 0.8469987228607918 and parameters: {'feature_selection__max_features': 406, 'classification__n_estimators': 960, 'classification__learning_rate': 0.009951752819460508, 'classification__max_depth': 7, 'classification__subsample': 0.7122077628301536, 'classification__colsample_bytree': 0.8632854012161579, 'classification__lambda': 0.24559605389073416, 'classification__alpha': 0.12985685147724232, 'classification__min_child_weight': 10}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  76%|███████▌  | 76/100 [13:52<05:39, 14.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:33:47,828] Trial 75 finished with value: 0.9134301765880712 and parameters: {'feature_selection__max_features': 514, 'classification__n_estimators': 906, 'classification__learning_rate': 0.0027270728131334227, 'classification__max_depth': 6, 'classification__subsample': 0.6837556283033474, 'classification__colsample_bytree': 0.9027682476040537, 'classification__lambda': 0.3850896931952597, 'classification__alpha': 0.11772810818989668, 'classification__min_child_weight': 10}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  77%|███████▋  | 77/100 [14:10<05:49, 15.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:34:05,426] Trial 76 finished with value: 0.9134301765880712 and parameters: {'feature_selection__max_features': 518, 'classification__n_estimators': 915, 'classification__learning_rate': 0.0025677523366448305, 'classification__max_depth': 7, 'classification__subsample': 0.6833738478401306, 'classification__colsample_bytree': 0.9176705774466115, 'classification__lambda': 0.3809648410801976, 'classification__alpha': 0.12698863540510505, 'classification__min_child_weight': 10}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  78%|███████▊  | 78/100 [14:32<06:17, 17.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:34:27,234] Trial 77 finished with value: 0.8966861598440546 and parameters: {'feature_selection__max_features': 622, 'classification__n_estimators': 1000, 'classification__learning_rate': 0.002282364747077922, 'classification__max_depth': 6, 'classification__subsample': 0.6319620631953055, 'classification__colsample_bytree': 0.8853574566252291, 'classification__lambda': 0.2501849767616327, 'classification__alpha': 0.20717602661095227, 'classification__min_child_weight': 10}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  79%|███████▉  | 79/100 [14:54<06:33, 18.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:34:49,655] Trial 78 finished with value: 0.8966861598440546 and parameters: {'feature_selection__max_features': 568, 'classification__n_estimators': 873, 'classification__learning_rate': 0.00282080537573438, 'classification__max_depth': 7, 'classification__subsample': 0.6446164676627316, 'classification__colsample_bytree': 0.9502311953196652, 'classification__lambda': 0.4809093552756564, 'classification__alpha': 0.11422922276146887, 'classification__min_child_weight': 10}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  80%|████████  | 80/100 [15:03<05:12, 15.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:34:57,983] Trial 79 finished with value: 0.8638650890792873 and parameters: {'feature_selection__max_features': 489, 'classification__n_estimators': 344, 'classification__learning_rate': 0.0021128034403392277, 'classification__max_depth': 7, 'classification__subsample': 0.7305510576261917, 'classification__colsample_bytree': 0.8338737799546801, 'classification__lambda': 0.3784388398590589, 'classification__alpha': 0.14300158274231214, 'classification__min_child_weight': 10}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  81%|████████  | 81/100 [15:25<05:38, 17.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:35:20,887] Trial 80 finished with value: 0.8802606434185382 and parameters: {'feature_selection__max_features': 677, 'classification__n_estimators': 936, 'classification__learning_rate': 0.002765031106364147, 'classification__max_depth': 6, 'classification__subsample': 0.709145320184872, 'classification__colsample_bytree': 0.8548520964713051, 'classification__lambda': 0.31115983526028496, 'classification__alpha': 0.1643536998148405, 'classification__min_child_weight': 10}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  82%|████████▏ | 82/100 [15:44<05:21, 17.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:35:38,951] Trial 81 finished with value: 0.8652666148128944 and parameters: {'feature_selection__max_features': 458, 'classification__n_estimators': 912, 'classification__learning_rate': 0.0025394060118077047, 'classification__max_depth': 7, 'classification__subsample': 0.807776970460397, 'classification__colsample_bytree': 0.9221409423215418, 'classification__lambda': 0.373257101376489, 'classification__alpha': 0.11603159920865681, 'classification__min_child_weight': 10}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  83%|████████▎ | 83/100 [16:01<05:01, 17.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:35:56,348] Trial 82 finished with value: 0.8966861598440546 and parameters: {'feature_selection__max_features': 518, 'classification__n_estimators': 957, 'classification__learning_rate': 0.002558593887881617, 'classification__max_depth': 8, 'classification__subsample': 0.6846218852391182, 'classification__colsample_bytree': 0.9016592731763557, 'classification__lambda': 0.16534989662133495, 'classification__alpha': 0.2564907428590224, 'classification__min_child_weight': 10}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  84%|████████▍ | 84/100 [16:19<04:46, 17.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:36:14,745] Trial 83 finished with value: 0.8969128246974023 and parameters: {'feature_selection__max_features': 520, 'classification__n_estimators': 910, 'classification__learning_rate': 0.0019824584935153287, 'classification__max_depth': 6, 'classification__subsample': 0.6693229495111995, 'classification__colsample_bytree': 0.9419769965001027, 'classification__lambda': 0.22625224795142462, 'classification__alpha': 0.2167895186894871, 'classification__min_child_weight': 9}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  85%|████████▌ | 85/100 [16:36<04:24, 17.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:36:31,701] Trial 84 finished with value: 0.8966861598440546 and parameters: {'feature_selection__max_features': 545, 'classification__n_estimators': 974, 'classification__learning_rate': 0.003000330528405661, 'classification__max_depth': 7, 'classification__subsample': 0.6839301235434989, 'classification__colsample_bytree': 0.8794974797401167, 'classification__lambda': 0.5971645804957, 'classification__alpha': 0.1703347306171338, 'classification__min_child_weight': 10}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  86%|████████▌ | 86/100 [16:52<04:00, 17.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:36:47,841] Trial 85 finished with value: 0.898177174039243 and parameters: {'feature_selection__max_features': 428, 'classification__n_estimators': 885, 'classification__learning_rate': 0.002241920056573177, 'classification__max_depth': 7, 'classification__subsample': 0.7030213591333527, 'classification__colsample_bytree': 0.8395822769081888, 'classification__lambda': 0.13226043212567068, 'classification__alpha': 0.10275425241433568, 'classification__min_child_weight': 9}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  87%|████████▋ | 87/100 [17:07<03:32, 16.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:37:02,179] Trial 86 finished with value: 0.8966861598440546 and parameters: {'feature_selection__max_features': 367, 'classification__n_estimators': 939, 'classification__learning_rate': 0.00243578966008259, 'classification__max_depth': 8, 'classification__subsample': 0.6246433842026835, 'classification__colsample_bytree': 0.9325736741470276, 'classification__lambda': 0.4424784994514212, 'classification__alpha': 0.15106215833071499, 'classification__min_child_weight': 10}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  88%|████████▊ | 88/100 [17:25<03:23, 16.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:37:20,506] Trial 87 finished with value: 0.898177174039243 and parameters: {'feature_selection__max_features': 475, 'classification__n_estimators': 859, 'classification__learning_rate': 0.002672851434064322, 'classification__max_depth': 9, 'classification__subsample': 0.6693588557229431, 'classification__colsample_bytree': 0.9745402641605214, 'classification__lambda': 0.1957049555065488, 'classification__alpha': 0.13277855218188858, 'classification__min_child_weight': 9}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  89%|████████▉ | 89/100 [17:41<03:03, 16.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:37:36,567] Trial 88 finished with value: 0.8969128246974023 and parameters: {'feature_selection__max_features': 497, 'classification__n_estimators': 971, 'classification__learning_rate': 0.00324631569356059, 'classification__max_depth': 6, 'classification__subsample': 0.7552868926483522, 'classification__colsample_bytree': 0.9089878970715431, 'classification__lambda': 0.26582784781055363, 'classification__alpha': 0.11973885478344405, 'classification__min_child_weight': 10}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  90%|█████████ | 90/100 [17:58<02:45, 16.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:37:52,953] Trial 89 finished with value: 0.9134301765880712 and parameters: {'feature_selection__max_features': 445, 'classification__n_estimators': 924, 'classification__learning_rate': 0.0029682619726881234, 'classification__max_depth': 5, 'classification__subsample': 0.6400747684944427, 'classification__colsample_bytree': 0.857037322043027, 'classification__lambda': 0.3094635634890357, 'classification__alpha': 0.1772305894249813, 'classification__min_child_weight': 9}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  91%|█████████ | 91/100 [18:12<02:23, 15.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:38:07,537] Trial 90 finished with value: 0.8802606434185382 and parameters: {'feature_selection__max_features': 527, 'classification__n_estimators': 888, 'classification__learning_rate': 0.0010481070319516101, 'classification__max_depth': 8, 'classification__subsample': 0.6158087530310107, 'classification__colsample_bytree': 0.8153111785649253, 'classification__lambda': 0.6349719351634623, 'classification__alpha': 0.25951130396576844, 'classification__min_child_weight': 10}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  92%|█████████▏| 92/100 [18:25<01:59, 14.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:38:20,113] Trial 91 finished with value: 0.9134301765880712 and parameters: {'feature_selection__max_features': 447, 'classification__n_estimators': 928, 'classification__learning_rate': 0.002893481548602573, 'classification__max_depth': 6, 'classification__subsample': 0.6408306963508756, 'classification__colsample_bytree': 0.8540803804536854, 'classification__lambda': 0.3443632296328377, 'classification__alpha': 0.17989601498755065, 'classification__min_child_weight': 9}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  93%|█████████▎| 93/100 [18:37<01:38, 14.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:38:31,928] Trial 92 finished with value: 0.9134301765880712 and parameters: {'feature_selection__max_features': 415, 'classification__n_estimators': 918, 'classification__learning_rate': 0.0027002575882076616, 'classification__max_depth': 5, 'classification__subsample': 0.6810929796843276, 'classification__colsample_bytree': 0.8237353067398374, 'classification__lambda': 0.30603369024202154, 'classification__alpha': 0.1573228299440932, 'classification__min_child_weight': 9}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  94%|█████████▍| 94/100 [18:49<01:21, 13.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:38:44,675] Trial 93 finished with value: 0.8966861598440546 and parameters: {'feature_selection__max_features': 472, 'classification__n_estimators': 837, 'classification__learning_rate': 0.0025321291503784355, 'classification__max_depth': 7, 'classification__subsample': 0.6107577083391067, 'classification__colsample_bytree': 0.8752077940510192, 'classification__lambda': 0.22890381123002299, 'classification__alpha': 0.22159682212398304, 'classification__min_child_weight': 10}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  95%|█████████▌| 95/100 [19:05<01:11, 14.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:39:00,368] Trial 94 finished with value: 0.8966861598440546 and parameters: {'feature_selection__max_features': 870, 'classification__n_estimators': 865, 'classification__learning_rate': 0.0030745831644591747, 'classification__max_depth': 5, 'classification__subsample': 0.6470033035331523, 'classification__colsample_bytree': 0.8496407113112261, 'classification__lambda': 0.41128928332540876, 'classification__alpha': 0.3657278671002793, 'classification__min_child_weight': 10}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  96%|█████████▌| 96/100 [19:20<00:57, 14.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:39:15,105] Trial 95 finished with value: 0.8817125064117848 and parameters: {'feature_selection__max_features': 338, 'classification__n_estimators': 983, 'classification__learning_rate': 0.0023657744105698864, 'classification__max_depth': 6, 'classification__subsample': 0.7029093917290478, 'classification__colsample_bytree': 0.896442040135418, 'classification__lambda': 0.8342904059182619, 'classification__alpha': 0.19048139234170702, 'classification__min_child_weight': 9}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  97%|█████████▋| 97/100 [19:31<00:40, 13.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:39:26,785] Trial 96 finished with value: 0.8966861598440546 and parameters: {'feature_selection__max_features': 555, 'classification__n_estimators': 817, 'classification__learning_rate': 0.0029080682215679257, 'classification__max_depth': 7, 'classification__subsample': 0.6649438658381099, 'classification__colsample_bytree': 0.8388732936891451, 'classification__lambda': 2.1836612063832734, 'classification__alpha': 0.10276745970521625, 'classification__min_child_weight': 10}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  98%|█████████▊| 98/100 [19:46<00:28, 14.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:39:41,849] Trial 97 finished with value: 0.8969128246974023 and parameters: {'feature_selection__max_features': 503, 'classification__n_estimators': 905, 'classification__learning_rate': 0.0032319918125749423, 'classification__max_depth': 8, 'classification__subsample': 0.7223435638569097, 'classification__colsample_bytree': 0.8627039138930391, 'classification__lambda': 0.1630232697193799, 'classification__alpha': 0.1240296146846582, 'classification__min_child_weight': 9}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343:  99%|█████████▉| 99/100 [20:02<00:14, 14.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:39:57,021] Trial 98 finished with value: 0.9134301765880712 and parameters: {'feature_selection__max_features': 439, 'classification__n_estimators': 944, 'classification__learning_rate': 0.00272390963386118, 'classification__max_depth': 7, 'classification__subsample': 0.6329066668018872, 'classification__colsample_bytree': 0.8898694427950256, 'classification__lambda': 0.28121231154720966, 'classification__alpha': 0.29358354007184545, 'classification__min_child_weight': 8}. Best is trial 43 with value: 0.9134301765880712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: 0.91343: 100%|██████████| 100/100 [20:15<00:00, 12.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 02:40:10,307] Trial 99 finished with value: 0.898177174039243 and parameters: {'feature_selection__max_features': 385, 'classification__n_estimators': 978, 'classification__learning_rate': 0.0030263248894225004, 'classification__max_depth': 5, 'classification__subsample': 0.6907378581201836, 'classification__colsample_bytree': 0.8029496154234576, 'classification__lambda': 0.3439484512232627, 'classification__alpha': 0.6510491322860879, 'classification__min_child_weight': 9}. Best is trial 43 with value: 0.9134301765880712.\n",
      "\n",
      "--- Optuna Search Complete ---\n",
      "Best validation F1-score: 0.9134301765880712\n",
      "\n",
      "--- Final Best Hyperparameters for Production Model ---\n",
      "{'feature_selection__max_features': 396, 'classification__n_estimators': 826, 'classification__learning_rate': 0.003050520176067708, 'classification__max_depth': 5, 'classification__subsample': 0.6186272156342054, 'classification__colsample_bytree': 0.7934754900372663, 'classification__lambda': 3.262422217867939, 'classification__alpha': 0.3223021165552474, 'classification__min_child_weight': 9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import optuna\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import f1_score\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "N_TRIALS = 100  \n",
    "VALIDATION_SET_SIZE = 0.2 \n",
    "\n",
    "# ==============================================================================\n",
    "\n",
    "def final_objective(trial, X_train_full, y_train_full):\n",
    "    params = {\n",
    "        'feature_selection__max_features': trial.suggest_int('feature_selection__max_features', 10, 1000),\n",
    "        'classification__n_estimators': trial.suggest_int('classification__n_estimators', 50, 1000),\n",
    "        'classification__learning_rate': trial.suggest_float('classification__learning_rate', 1e-3, 0.01, log=True),\n",
    "        'classification__max_depth': trial.suggest_int('classification__max_depth', 3, 15),\n",
    "        'classification__subsample': trial.suggest_float('classification__subsample', 0.6, 1.0),\n",
    "        'classification__colsample_bytree': trial.suggest_float('classification__colsample_bytree', 0.6, 1.0),\n",
    "        'classification__lambda': trial.suggest_float('classification__lambda', 0.1, 30.0, log=True),\n",
    "        'classification__alpha': trial.suggest_float('classification__alpha', 0.1, 20.0, log=True),\n",
    "        'classification__min_child_weight': trial.suggest_int('classification__min_child_weight', 1, 10)\n",
    "    }\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('feature_selection', SelectFromModel(RandomForestClassifier(random_state=42))),\n",
    "        ('classification', XGBClassifier(random_state=42, eval_metric='mlogloss')) \n",
    "    ])\n",
    "    pipeline.set_params(**params)\n",
    "\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=VALIDATION_SET_SIZE, random_state=42)\n",
    "    train_idx, val_idx = next(sss.split(X_train_full, y_train_full))\n",
    "\n",
    "    X_train, X_val = X_train_full.iloc[train_idx], X_train_full.iloc[val_idx]\n",
    "    y_train, y_val = y_train_full[train_idx], y_train_full[val_idx]\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    predictions = pipeline.predict(X_val)\n",
    "    score = f1_score(y_val, predictions, average='macro')\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    X_train_full, y_train_full = create_sub_writers_bootstrap(\n",
    "        train_df=all_writers_info,\n",
    "        feature_store=feature_store,\n",
    "        images_per_subwriter=4,\n",
    "        fold_seed=42 \n",
    "    )\n",
    "    print(f\"Augmented training set created with {len(X_train_full)} samples.\")\n",
    "\n",
    "    print(f\"\\nStarting Optuna hyperparameter search for {N_TRIALS} trials...\")\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    \n",
    "    objective_with_data = lambda trial: final_objective(trial, X_train_full, y_train_full)\n",
    "    \n",
    "    study.optimize(objective_with_data, n_trials=N_TRIALS, show_progress_bar=True)\n",
    "\n",
    "    final_best_params = study.best_params\n",
    "    print(\"\\n--- Optuna Search Complete ---\")\n",
    "    print(f\"Best validation F1-score: {study.best_value}\")\n",
    "    print(\"\\n--- Final Best Hyperparameters for Production Model ---\")\n",
    "    print(final_best_params)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75f73fe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()), (&#x27;smote&#x27;, SMOTE(random_state=42)),\n",
       "                (&#x27;feature_selection&#x27;,\n",
       "                 SelectFromModel(estimator=RandomForestClassifier(random_state=42),\n",
       "                                 max_features=396)),\n",
       "                (&#x27;classification&#x27;,\n",
       "                 XGBClassifier(alpha=0.3223021165552474, base_score=None,\n",
       "                               booster=None, callbacks=None,\n",
       "                               colsample_bylevel=None, colsample_bynode=None,\n",
       "                               colsample_bytree=0.79...\n",
       "                               feature_types=None, feature_weights=None,\n",
       "                               gamma=None, grow_policy=None,\n",
       "                               importance_type=None,\n",
       "                               interaction_constraints=None,\n",
       "                               lambda=3.262422217867939,\n",
       "                               learning_rate=0.003050520176067708, max_bin=None,\n",
       "                               max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "                               max_delta_step=None, max_depth=5,\n",
       "                               max_leaves=None, min_child_weight=9, missing=nan,\n",
       "                               monotone_constraints=None, multi_strategy=None,\n",
       "                               n_estimators=826, ...))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label  sk-toggleable__label-arrow\"><div><div>Pipeline</div></div><div><span class=\"sk-estimator-doc-link \">i<span>Not fitted</span></span></div></label><div class=\"sk-toggleable__content \"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()), (&#x27;smote&#x27;, SMOTE(random_state=42)),\n",
       "                (&#x27;feature_selection&#x27;,\n",
       "                 SelectFromModel(estimator=RandomForestClassifier(random_state=42),\n",
       "                                 max_features=396)),\n",
       "                (&#x27;classification&#x27;,\n",
       "                 XGBClassifier(alpha=0.3223021165552474, base_score=None,\n",
       "                               booster=None, callbacks=None,\n",
       "                               colsample_bylevel=None, colsample_bynode=None,\n",
       "                               colsample_bytree=0.79...\n",
       "                               feature_types=None, feature_weights=None,\n",
       "                               gamma=None, grow_policy=None,\n",
       "                               importance_type=None,\n",
       "                               interaction_constraints=None,\n",
       "                               lambda=3.262422217867939,\n",
       "                               learning_rate=0.003050520176067708, max_bin=None,\n",
       "                               max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "                               max_delta_step=None, max_depth=5,\n",
       "                               max_leaves=None, min_child_weight=9, missing=nan,\n",
       "                               monotone_constraints=None, multi_strategy=None,\n",
       "                               n_estimators=826, ...))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label  sk-toggleable__label-arrow\"><div><div>StandardScaler</div></div><div><a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a></div></label><div class=\"sk-toggleable__content \"><pre>StandardScaler()</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label  sk-toggleable__label-arrow\"><div><div>SMOTE</div></div></label><div class=\"sk-toggleable__content \"><pre>SMOTE(random_state=42)</pre></div> </div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label  sk-toggleable__label-arrow\"><div><div>feature_selection: SelectFromModel</div></div><div><a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.feature_selection.SelectFromModel.html\">?<span>Documentation for feature_selection: SelectFromModel</span></a></div></label><div class=\"sk-toggleable__content \"><pre>SelectFromModel(estimator=RandomForestClassifier(random_state=42),\n",
       "                max_features=396)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label  sk-toggleable__label-arrow\"><div><div>estimator: RandomForestClassifier</div></div></label><div class=\"sk-toggleable__content \"><pre>RandomForestClassifier(random_state=42)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label  sk-toggleable__label-arrow\"><div><div>RandomForestClassifier</div></div><div><a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a></div></label><div class=\"sk-toggleable__content \"><pre>RandomForestClassifier(random_state=42)</pre></div> </div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label  sk-toggleable__label-arrow\"><div><div>XGBClassifier</div></div><div><a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.0.0/python/python_api.html#xgboost.XGBClassifier\">?<span>Documentation for XGBClassifier</span></a></div></label><div class=\"sk-toggleable__content \"><pre>XGBClassifier(alpha=0.3223021165552474, base_score=None, booster=None,\n",
       "              callbacks=None, colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.7934754900372663, device=None,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=&#x27;mlogloss&#x27;, feature_types=None, feature_weights=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, lambda=3.262422217867939,\n",
       "              learning_rate=0.003050520176067708, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=5, max_leaves=None,\n",
       "              min_child_weight=9, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=826, ...)</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()), ('smote', SMOTE(random_state=42)),\n",
       "                ('feature_selection',\n",
       "                 SelectFromModel(estimator=RandomForestClassifier(random_state=42),\n",
       "                                 max_features=396)),\n",
       "                ('classification',\n",
       "                 XGBClassifier(alpha=0.3223021165552474, base_score=None,\n",
       "                               booster=None, callbacks=None,\n",
       "                               colsample_bylevel=None, colsample_bynode=None,\n",
       "                               colsample_bytree=0.79...\n",
       "                               feature_types=None, feature_weights=None,\n",
       "                               gamma=None, grow_policy=None,\n",
       "                               importance_type=None,\n",
       "                               interaction_constraints=None,\n",
       "                               lambda=3.262422217867939,\n",
       "                               learning_rate=0.003050520176067708, max_bin=None,\n",
       "                               max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "                               max_delta_step=None, max_depth=5,\n",
       "                               max_leaves=None, min_child_weight=9, missing=nan,\n",
       "                               monotone_constraints=None, multi_strategy=None,\n",
       "                               n_estimators=826, ...))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "import optuna\n",
    "from optuna.distributions import FloatDistribution, IntDistribution\n",
    "# from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import joblib\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('smote', SMOTE(random_state=42)),\n",
    "            ('feature_selection', SelectFromModel(RandomForestClassifier(random_state=42))),\n",
    "            ('classification', XGBClassifier(random_state=42, eval_metric='mlogloss'))\n",
    "        ])\n",
    "\n",
    "\n",
    "best_hyperparameters = final_best_params\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pipeline.set_params(**best_hyperparameters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3491f44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 3 4 5 6]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0|mean</th>\n",
       "      <th>0|median</th>\n",
       "      <th>0|std</th>\n",
       "      <th>0|skew</th>\n",
       "      <th>1|mean</th>\n",
       "      <th>1|median</th>\n",
       "      <th>1|std</th>\n",
       "      <th>1|skew</th>\n",
       "      <th>2|mean</th>\n",
       "      <th>2|median</th>\n",
       "      <th>...</th>\n",
       "      <th>637|std</th>\n",
       "      <th>637|skew</th>\n",
       "      <th>638|mean</th>\n",
       "      <th>638|median</th>\n",
       "      <th>638|std</th>\n",
       "      <th>638|skew</th>\n",
       "      <th>639|mean</th>\n",
       "      <th>639|median</th>\n",
       "      <th>639|std</th>\n",
       "      <th>639|skew</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.040413</td>\n",
       "      <td>1.280957</td>\n",
       "      <td>0.507247</td>\n",
       "      <td>-1.634625</td>\n",
       "      <td>-1.159289</td>\n",
       "      <td>-1.241885</td>\n",
       "      <td>0.464899</td>\n",
       "      <td>0.226526</td>\n",
       "      <td>1.880857</td>\n",
       "      <td>2.063291</td>\n",
       "      <td>...</td>\n",
       "      <td>0.694412</td>\n",
       "      <td>-0.121321</td>\n",
       "      <td>-0.798057</td>\n",
       "      <td>-0.891258</td>\n",
       "      <td>0.593199</td>\n",
       "      <td>1.456927</td>\n",
       "      <td>2.265608</td>\n",
       "      <td>2.281312</td>\n",
       "      <td>0.613156</td>\n",
       "      <td>-0.070073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.934578</td>\n",
       "      <td>-0.684685</td>\n",
       "      <td>0.876675</td>\n",
       "      <td>-0.531585</td>\n",
       "      <td>-1.353505</td>\n",
       "      <td>-1.126178</td>\n",
       "      <td>0.485313</td>\n",
       "      <td>-0.955035</td>\n",
       "      <td>-0.034474</td>\n",
       "      <td>-0.115520</td>\n",
       "      <td>...</td>\n",
       "      <td>1.043964</td>\n",
       "      <td>0.250822</td>\n",
       "      <td>-0.734064</td>\n",
       "      <td>-0.966799</td>\n",
       "      <td>0.707600</td>\n",
       "      <td>1.696434</td>\n",
       "      <td>-0.382077</td>\n",
       "      <td>-0.125341</td>\n",
       "      <td>0.555364</td>\n",
       "      <td>-0.791461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.588434</td>\n",
       "      <td>-0.489132</td>\n",
       "      <td>0.626406</td>\n",
       "      <td>0.217603</td>\n",
       "      <td>-0.098024</td>\n",
       "      <td>-0.090292</td>\n",
       "      <td>0.398646</td>\n",
       "      <td>0.237709</td>\n",
       "      <td>-0.224826</td>\n",
       "      <td>-0.164748</td>\n",
       "      <td>...</td>\n",
       "      <td>0.706273</td>\n",
       "      <td>-0.112840</td>\n",
       "      <td>-0.673228</td>\n",
       "      <td>-0.783320</td>\n",
       "      <td>0.388836</td>\n",
       "      <td>0.768802</td>\n",
       "      <td>1.231104</td>\n",
       "      <td>1.344649</td>\n",
       "      <td>0.723773</td>\n",
       "      <td>-0.476075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.085146</td>\n",
       "      <td>0.121908</td>\n",
       "      <td>0.921798</td>\n",
       "      <td>-0.473916</td>\n",
       "      <td>0.415825</td>\n",
       "      <td>0.346491</td>\n",
       "      <td>0.712960</td>\n",
       "      <td>0.447135</td>\n",
       "      <td>1.307174</td>\n",
       "      <td>1.396539</td>\n",
       "      <td>...</td>\n",
       "      <td>0.875910</td>\n",
       "      <td>-1.390163</td>\n",
       "      <td>-1.015302</td>\n",
       "      <td>-0.808681</td>\n",
       "      <td>0.640124</td>\n",
       "      <td>-1.573374</td>\n",
       "      <td>1.290129</td>\n",
       "      <td>1.042611</td>\n",
       "      <td>0.776999</td>\n",
       "      <td>1.529961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.493525</td>\n",
       "      <td>-0.451743</td>\n",
       "      <td>0.309951</td>\n",
       "      <td>-0.359937</td>\n",
       "      <td>-0.484127</td>\n",
       "      <td>-0.508899</td>\n",
       "      <td>0.491227</td>\n",
       "      <td>0.855808</td>\n",
       "      <td>-0.000721</td>\n",
       "      <td>0.027293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.550214</td>\n",
       "      <td>-0.093284</td>\n",
       "      <td>-0.666505</td>\n",
       "      <td>-0.790870</td>\n",
       "      <td>0.448327</td>\n",
       "      <td>2.162568</td>\n",
       "      <td>0.404768</td>\n",
       "      <td>0.603208</td>\n",
       "      <td>0.371432</td>\n",
       "      <td>-1.817523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.929685</td>\n",
       "      <td>-0.900334</td>\n",
       "      <td>0.618008</td>\n",
       "      <td>-0.059490</td>\n",
       "      <td>-1.082905</td>\n",
       "      <td>-0.961740</td>\n",
       "      <td>0.465567</td>\n",
       "      <td>-1.370025</td>\n",
       "      <td>0.395181</td>\n",
       "      <td>0.418887</td>\n",
       "      <td>...</td>\n",
       "      <td>0.405486</td>\n",
       "      <td>-0.012981</td>\n",
       "      <td>-0.542955</td>\n",
       "      <td>-0.543840</td>\n",
       "      <td>0.459550</td>\n",
       "      <td>0.008677</td>\n",
       "      <td>-0.114262</td>\n",
       "      <td>0.064019</td>\n",
       "      <td>0.611162</td>\n",
       "      <td>-1.255014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 2560 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0|mean  0|median     0|std    0|skew    1|mean  1|median     1|std  \\\n",
       "0  1.040413  1.280957  0.507247 -1.634625 -1.159289 -1.241885  0.464899   \n",
       "1 -0.934578 -0.684685  0.876675 -0.531585 -1.353505 -1.126178  0.485313   \n",
       "2 -0.588434 -0.489132  0.626406  0.217603 -0.098024 -0.090292  0.398646   \n",
       "3 -0.085146  0.121908  0.921798 -0.473916  0.415825  0.346491  0.712960   \n",
       "4 -0.493525 -0.451743  0.309951 -0.359937 -0.484127 -0.508899  0.491227   \n",
       "5 -0.929685 -0.900334  0.618008 -0.059490 -1.082905 -0.961740  0.465567   \n",
       "\n",
       "     1|skew    2|mean  2|median  ...   637|std  637|skew  638|mean  \\\n",
       "0  0.226526  1.880857  2.063291  ...  0.694412 -0.121321 -0.798057   \n",
       "1 -0.955035 -0.034474 -0.115520  ...  1.043964  0.250822 -0.734064   \n",
       "2  0.237709 -0.224826 -0.164748  ...  0.706273 -0.112840 -0.673228   \n",
       "3  0.447135  1.307174  1.396539  ...  0.875910 -1.390163 -1.015302   \n",
       "4  0.855808 -0.000721  0.027293  ...  0.550214 -0.093284 -0.666505   \n",
       "5 -1.370025  0.395181  0.418887  ...  0.405486 -0.012981 -0.542955   \n",
       "\n",
       "   638|median   638|std  638|skew  639|mean  639|median   639|std  639|skew  \n",
       "0   -0.891258  0.593199  1.456927  2.265608    2.281312  0.613156 -0.070073  \n",
       "1   -0.966799  0.707600  1.696434 -0.382077   -0.125341  0.555364 -0.791461  \n",
       "2   -0.783320  0.388836  0.768802  1.231104    1.344649  0.723773 -0.476075  \n",
       "3   -0.808681  0.640124 -1.573374  1.290129    1.042611  0.776999  1.529961  \n",
       "4   -0.790870  0.448327  2.162568  0.404768    0.603208  0.371432 -1.817523  \n",
       "5   -0.543840  0.459550  0.008677 -0.114262    0.064019  0.611162 -1.255014  \n",
       "\n",
       "[6 rows x 2560 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_test, y_test = create_writer_level_profile(real_world_data, all_features)\n",
    "display(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99156986",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train_full, y_train_full)\n",
    "\n",
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e72c15d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08022325, 0.16815141, 0.75162536],\n",
       "       [0.0715434 , 0.08215123, 0.84630543],\n",
       "       [0.06525245, 0.28027517, 0.6544724 ],\n",
       "       [0.06573303, 0.0849935 , 0.8492735 ],\n",
       "       [0.18327358, 0.13533378, 0.6813926 ],\n",
       "       [0.08279838, 0.07310566, 0.84409595]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dba1bebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[[0 0 3]\n",
      " [0 0 3]\n",
      " [0 0 0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(type(cm))\n",
    "print(cm)\n",
    "cm_disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['German', 'French', 'English']).plot(cmap='viridis', values_format='d')\n",
    "cm_disp\n",
    "\n",
    "colorbar = cm_disp.im_.colorbar\n",
    "colorbar.set_ticks([0, 1, 2, 3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ba593e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
